{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"ff3c9fb9-0c86-4acf-9162-c741c46e53a4","_kg_hide-output":false,"_uuid":"91db1348-a896-4607-8686-f6c6df6419ed","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:30:35.202851Z","iopub.status.busy":"2023-03-19T07:30:35.202431Z","iopub.status.idle":"2023-03-19T07:30:53.178442Z","shell.execute_reply":"2023-03-19T07:30:53.177132Z","shell.execute_reply.started":"2023-03-19T07:30:35.202813Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","from torch.utils.data import ConcatDataset\n","from torch.cuda.amp import autocast, GradScaler\n","import os\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import warnings\n","import gc\n","import cv2\n","# 忽略所有警告\n","warnings.filterwarnings('ignore')\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","seed_value = 42   # 设定随机数种子\n","\n","np.random.seed(seed_value)\n","random.seed(seed_value)\n","os.environ['PYTHONHASHSEED'] = str(seed_value)  # 为了禁止hash随机化，使得实验可复现。\n","\n","torch.manual_seed(seed_value)     # 为CPU设置随机种子\n","torch.cuda.manual_seed(seed_value)      # 为当前GPU设置随机种子（只用一块GPU）\n","torch.cuda.manual_seed_all(seed_value)   # 为所有GPU设置随机种子（多块GPU）\n","\n","torch.backends.cudnn.deterministic = True\n","\n","class CFG:\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    checpoint = 'result/ConvNeXt_Uper-DIM-10-[eval_loss]-0.3738-[dice_score]-0.45-19-epoch.pkl'\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","\n","    # # comp_dir_path = './'\n","    # comp_dir_path = '/kaggle/input/'\n","    # comp_folder_name = 'vesuvius-challenge-ink-detection'\n","    # # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    # comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","        # comp_dir_path = './'\n","    comp_dir_path = ''\n","    comp_folder_name = 'data'\n","    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","    \n","    exp_name = 'Unet_stride'\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'Unet'\n","\n","    in_chans = 24# 65\n","    # ============== training cfg =============\n","    size = 224\n","    tile_size = 224\n","    stride = tile_size // 2\n","\n","    train_batch_size = 16 # 32\n","    valid_batch_size = 16\n","    use_amp = True\n","\n","    epochs = 30 # 30\n","\n","    # lr = 1e-4 / warmup_factor\n","    lr = 1e-4\n","\n","    # ============== fixed =============\n","    pretrained = True\n","\n","    backbone = 'se_resnext50_32x4d'\n","\n","    min_lr = 1e-6\n","    weight_decay = 1e-6\n","    max_grad_norm = 1000\n","\n","    num_workers = 4\n","\n","    seed = 42\n","\n","    threshhold = 0.5\n","\n","    all_best_dice = 0\n","    all_best_loss = np.float('inf')\n","\n","    shape_list = []\n","    test_shape_list = []\n","\n","    val_mask = None\n","    val_label = None\n","\n","    # ============== set dataset path =============\n","\n","    # outputs_path = f'/kaggle/working/outputs/{comp_name}/{exp_name}/'\n","    outputs_path = 'result/'\n","\n","    submission_dir = outputs_path + 'submissions/'\n","    submission_path = submission_dir + f'submission_{exp_name}.csv'\n","\n","    model_dir = outputs_path\n","    log_dir = outputs_path + 'logs/'\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        # A.RandomResizedCrop(\n","        #     size, size, scale=(0.85, 1.0)),\n","        A.Resize(size, size),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.75),\n","        A.ShiftScaleRotate(p=0.75),\n","        A.OneOf([\n","                A.GaussNoise(var_limit=[10, 50]),\n","                A.GaussianBlur(),\n","                A.MotionBlur(),\n","                ], p=0.4),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                        mask_fill_value=0, p=0.5),\n","        # A.Cutout(max_h_size=int(size * 0.6),\n","        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n","        A.Normalize(\n","            mean= [0] * in_chans,\n","            std= [1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","\n","    valid_aug_list = [\n","        A.Resize(size, size),\n","        A.Normalize(\n","            mean= [0] * in_chans,\n","            std= [1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","seed = CFG.seed\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_image_mask(fragment_id):\n","\n","    images = []\n","\n","    # idxs = range(65)\n","    mid = 65 // 2\n","    start = mid - CFG.in_chans // 2\n","    end = mid + CFG.in_chans // 2\n","    idxs = range(start, end)\n","\n","    for i in tqdm(idxs):\n","        \n","        image = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n","\n","        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size + 1)\n","        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size + 1)\n","\n","        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","        images.append(image)\n","    images = np.stack(images, axis=2)\n","\n","    label = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/inklabels.png\", 0)\n","    label = np.pad(label, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    label = label.astype('float32')\n","    label /= 255.0\n","\n","    mask = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/mask.png\", 0)\n","    mask = np.pad(mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    mask = (mask / 255.).astype('float32')\n","\n","    CFG.shape_list.append(mask.shape)\n","    if fragment_id == 1:\n","        CFG.val_mask = mask\n","        CFG.val_label = label\n","    \n","    return images, label, mask\n","\n","def get_train_valid_dataset(val_persent=0.05):\n","    train_id = [2, 3]\n","    train_images = []\n","    train_labels = []\n","\n","    valid_images = []\n","    valid_labels = []\n","    valid_positons = []\n","\n","    for fragment_id in range(1, 4):\n","\n","        image, label, mask = read_image_mask(fragment_id)\n","\n","        x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n","        y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n","\n","        for y1 in y1_list:\n","            for x1 in x1_list:\n","                y2 = y1 + CFG.tile_size\n","                x2 = x1 + CFG.tile_size\n","                if sum(list(mask[y1:y2, x1:x2].flatten())) != 0:\n","                    if fragment_id in train_id:\n","                        train_images.append(image[y1:y2, x1:x2])\n","                        train_labels.append(label[y1:y2, x1:x2, None])\n","                    else:\n","                        valid_images.append(image[y1:y2, x1:x2])\n","                        valid_labels.append(label[y1:y2, x1:x2, None])\n","                        valid_positons.append([x1, y1, x2, y2, fragment_id - 1])\n","    return train_images, train_labels, valid_images, valid_labels, valid_positons\n","\n","def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","    return aug\n","\n","class SubvolumeDataset(data.Dataset):\n","    def __init__(self, images, labels, positions, transform, is_train):\n","        self.transform = transform\n","        self.images = images\n","        self.labels = labels\n","        self.is_train = is_train\n","        self.positions = positions\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, index):\n","        if self.is_train:\n","            image = self.images[index]\n","            label = self.labels[index]\n","            if self.positions:\n","                position = np.array(self.positions[index])\n","            else:\n","                position = np.zeros(1)\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","                label = data['mask']\n","            return image, label, position\n","        else:\n","            image = self.images[index]\n","            position = np.array(self.positions[index])\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","            return image, position\n","        \n","# IOU and Dice Score\n","def dice_coef(targets, preds, thr=0.5, beta=0.5, smooth=1e-5):\n","\n","    #comment out if your model contains a sigmoid or equivalent activation layer\n","    # flatten label and prediction tensors\n","    preds = (preds > thr).view(-1).float()\n","    targets = targets.view(-1).float()\n","\n","    y_true_count = targets.sum()\n","    ctp = preds[targets==1].sum()\n","    cfp = preds[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 24/24 [00:21<00:00,  1.14it/s]\n","100%|██████████| 24/24 [01:12<00:00,  3.00s/it]\n","100%|██████████| 24/24 [00:06<00:00,  3.54it/s]\n"]}],"source":["train_images, train_labels, valid_images, valid_labels, valid_positons = get_train_valid_dataset()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_dataset = SubvolumeDataset(train_images, train_labels, None,get_transforms(data='train', cfg=CFG), True)\n","valid_dataset = SubvolumeDataset(valid_images, valid_labels, valid_positons, get_transforms(data='valid', cfg=CFG), True)\n","train_loader = data.DataLoader(train_dataset,\n","                          batch_size=CFG.train_batch_size,\n","                          shuffle=True,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n","                          )\n","valid_loader = data.DataLoader(valid_dataset,\n","                          batch_size=CFG.valid_batch_size,\n","                          shuffle=False,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from functools import partial\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import trunc_normal_, DropPath\n","from mmcv.utils import get_logger\n","import logging\n","from mmcv.cnn import ConvModule\n","\n","def resize(input,\n","           size=None,\n","           scale_factor=None,\n","           mode='nearest',\n","           align_corners=None,\n","           warning=True):\n","    if warning:\n","        if size is not None and align_corners:\n","            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n","            output_h, output_w = tuple(int(x) for x in size)\n","            if output_h > input_h or output_w > output_h:\n","                if ((output_h > 1 and output_w > 1 and input_h > 1\n","                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n","                        and (output_w - 1) % (input_w - 1)):\n","                    warnings.warn(\n","                        f'When align_corners={align_corners}, '\n","                        'the output would more aligned if '\n","                        f'input size {(input_h, input_w)} is `x+1` and '\n","                        f'out size {(output_h, output_w)} is `nx+1`')\n","    return F.interpolate(input, size, scale_factor, mode, align_corners)\n","\n","def get_root_logger(log_file=None, log_level=logging.INFO):\n","    \"\"\"Get the root logger.\n","\n","    The logger will be initialized if it has not been initialized. By default a\n","    StreamHandler will be added. If `log_file` is specified, a FileHandler will\n","    also be added. The name of the root logger is the top-level package name,\n","    e.g., \"mmseg\".\n","\n","    Args:\n","        log_file (str | None): The log filename. If specified, a FileHandler\n","            will be added to the root logger.\n","        log_level (int): The root logger level. Note that only the process of\n","            rank 0 is affected, while other processes will set the level to\n","            \"Error\" and be silent most of the time.\n","\n","    Returns:\n","        logging.Logger: The root logger.\n","    \"\"\"\n","\n","    logger = get_logger(name='mmseg', log_file=log_file, log_level=log_level)\n","\n","    return logger\n","\n","class BayarConv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding=2):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.minus1 = (torch.ones(self.in_channels, self.out_channels, 1) * -1.000)\n","\n","        super(BayarConv2d, self).__init__()\n","        # only (kernel_size ** 2 - 1) trainable params as the center element is always -1\n","        self.kernel = nn.Parameter(torch.rand(self.in_channels, self.out_channels, kernel_size ** 2 - 1),\n","                                   requires_grad=True)\n","\n","\n","    def bayarConstraint(self):\n","        self.kernel.data = self.kernel.permute(2, 0, 1)\n","        self.kernel.data = torch.div(self.kernel.data, self.kernel.data.sum(0))\n","        self.kernel.data = self.kernel.permute(1, 2, 0)\n","        ctr = self.kernel_size ** 2 // 2\n","        real_kernel = torch.cat((self.kernel[:, :, :ctr], self.minus1.to(self.kernel.device), self.kernel[:, :, ctr:]), dim=2)\n","        real_kernel = real_kernel.reshape((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n","        return real_kernel\n","\n","    def forward(self, x):\n","        x = F.conv2d(x, self.bayarConstraint(), stride=self.stride, padding=self.padding)\n","        return x\n","\n","class Block(nn.Module):\n","    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n","    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n","    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n","    We use (2) as we find it slightly faster in PyTorch\n","    \n","    Args:\n","        dim (int): Number of input channels.\n","        drop_path (float): Stochastic depth rate. Default: 0.0\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","    \"\"\"\n","    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n","        super().__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n","        self.norm = LayerNorm(dim, eps=1e-6)\n","        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n","        self.act = nn.GELU()\n","        self.pwconv2 = nn.Linear(4 * dim, dim)\n","        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n","                                    requires_grad=True) if layer_scale_init_value > 0 else None\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.dwconv(x)\n","        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n","        x = self.norm(x)\n","        x = self.pwconv1(x)\n","        x = self.act(x)\n","        x = self.pwconv2(x)\n","        if self.gamma is not None:\n","            x = self.gamma * x\n","        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n","\n","        x = input + self.drop_path(x)\n","        return x\n","\n","class LayerNorm(nn.Module):\n","    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n","    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n","    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n","    with shape (batch_size, channels, height, width).\n","    \"\"\"\n","    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.data_format = data_format\n","        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n","            raise NotImplementedError \n","        self.normalized_shape = (normalized_shape, )\n","    \n","    def forward(self, x):\n","        if self.data_format == \"channels_last\":\n","            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.data_format == \"channels_first\":\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x\n","\n","class ConvNeXt(nn.Module):\n","    r\"\"\" ConvNeXt\n","        A PyTorch impl of : `A ConvNet for the 2020s`  -\n","          https://arxiv.org/pdf/2201.03545.pdf\n","\n","    Args:\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n","        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n","    \"\"\"\n","    def __init__(self, in_chans=3, bayar=False, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], \n","                 drop_path_rate=0., layer_scale_init_value=1e-6, out_indices=[0, 1, 2, 3], pretrained=None\n","                 ):\n","        super().__init__()\n","        self.pretrained = pretrained\n","        self.in_chans = in_chans\n","        self.bayar = bayar\n","        if self.bayar:\n","            self.bayar_conv = BayarConv2d(in_chans, in_chans)\n","            self.in_chans = in_chans * 2\n","\n","        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n","        stem = nn.Sequential(\n","            nn.Conv2d(self.in_chans, dims[0], kernel_size=4, stride=4),\n","            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n","        )\n","        self.downsample_layers.append(stem)\n","        for i in range(len(dims) - 1):\n","            downsample_layer = nn.Sequential(\n","                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n","                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n","        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n","        cur = 0\n","        for i in range(len(dims)):\n","            stage = nn.Sequential(\n","                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n","                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n","            )\n","            self.stages.append(stage)\n","            cur += depths[i]\n","\n","        self.out_indices = out_indices\n","\n","        norm_layer = partial(LayerNorm, eps=1e-6, data_format=\"channels_first\")\n","        for i_layer in range(len(dims)):\n","            layer = norm_layer(dims[i_layer])\n","            layer_name = f'norm{i_layer}'\n","            self.add_module(layer_name, layer)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, (nn.Conv2d, nn.Linear)):\n","            trunc_normal_(m.weight, std=.02)\n","            nn.init.constant_(m.bias, 0)\n","\n","    def init_weights(self, pretrained=None):\n","        \"\"\"Initialize the weights in backbone.\n","        Args:\n","            pretrained (str, optional): Path to pre-trained weights.\n","                Defaults to None.\n","        \"\"\"\n","        if pretrained is None:\n","            pretrained = self.pretrained\n","\n","        def _init_weights(m):\n","            if isinstance(m, nn.Linear):\n","                trunc_normal_(m.weight, std=.02)\n","                if isinstance(m, nn.Linear) and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.LayerNorm):\n","                nn.init.constant_(m.bias, 0)\n","                nn.init.constant_(m.weight, 1.0)\n","\n","        if isinstance(pretrained, str):\n","            self.apply(_init_weights)\n","            logger = get_root_logger()\n","        elif pretrained is None:\n","            self.apply(_init_weights)\n","        else:\n","            raise TypeError('pretrained must be a str or None')\n","\n","    def forward_features(self, x):\n","        if self.bayar:\n","            x_bayar = self.bayar_conv(x)\n","            x = torch.cat([x, x_bayar], dim=1)\n","        outs = []\n","        for i in range(4):\n","            x = self.downsample_layers[i](x)\n","            x = self.stages[i](x)\n","            if i in self.out_indices:\n","                norm_layer = getattr(self, f'norm{i}')\n","                x_out = norm_layer(x)\n","                outs.append(x_out)\n","\n","        return tuple(outs)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        return x\n","    \n","class PPM(nn.ModuleList):\n","    \"\"\"Pooling Pyramid Module used in PSPNet.\n","\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module.\n","        in_channels (int): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        conv_cfg (dict|None): Config of conv layers.\n","        norm_cfg (dict|None): Config of norm layers.\n","        act_cfg (dict): Config of activation layers.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","    \"\"\"\n","\n","    def __init__(self, pool_scales, in_channels, channels, conv_cfg, norm_cfg,\n","                 act_cfg, align_corners, **kwargs):\n","        super(PPM, self).__init__()\n","        self.pool_scales = pool_scales\n","        self.align_corners = align_corners\n","        self.in_channels = in_channels\n","        self.channels = channels\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        for pool_scale in pool_scales:\n","            self.append(\n","                nn.Sequential(\n","                    nn.AdaptiveAvgPool2d(pool_scale),\n","                    ConvModule(\n","                        self.in_channels,\n","                        self.channels,\n","                        1,\n","                        conv_cfg=self.conv_cfg,\n","                        norm_cfg=self.norm_cfg,\n","                        act_cfg=self.act_cfg,\n","                        **kwargs)))\n","\n","    def forward(self, x):\n","        \"\"\"Forward function.\"\"\"\n","        ppm_outs = []\n","        for ppm in self:\n","            ppm_out = ppm(x)\n","            upsampled_ppm_out = resize(\n","                ppm_out,\n","                size=x.size()[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","            ppm_outs.append(upsampled_ppm_out)\n","        return ppm_outs\n","\n","class UPerHead(nn.Module):\n","    \"\"\"Unified Perceptual Parsing for Scene Understanding.\n","\n","    This head is the implementation of `UPerNet\n","    <https://arxiv.org/abs/1807.10221>`_.\n","\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module applied on the last feature. Default: (1, 2, 3, 6).\n","    \"\"\"\n","\n","    def __init__(self, \n","                in_channels=[128, 256, 512, 1024],\n","                in_index=[0, 1, 2, 3],\n","                pool_scales=(1, 2, 3, 6),\n","                channels=512,\n","                dropout_ratio=0.1,\n","                num_classes=0,\n","                norm_cfg=dict(type='BN', requires_grad=True),\n","                align_corners=False):\n","        super(UPerHead, self).__init__()\n","        # PSP Module\n","        self.input_transform = 'multiple_select'\n","        self.align_corners = align_corners\n","        self.in_index = in_index\n","        self.psp_modules = PPM(\n","            pool_scales,\n","            in_channels[-1],\n","            channels,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'),\n","            align_corners=align_corners)\n","        self.bottleneck = ConvModule(\n","            in_channels[-1] + len(pool_scales) * channels,\n","            channels,\n","            3,\n","            padding=1,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'))\n","        # FPN Module\n","        self.lateral_convs = nn.ModuleList()\n","        self.fpn_convs = nn.ModuleList()\n","        for in_channel in in_channels[:-1]:  # skip the top layer\n","            l_conv = ConvModule(\n","                in_channel,\n","                channels,\n","                1,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU'),\n","                inplace=False)\n","            fpn_conv = ConvModule(\n","                channels,\n","                channels,\n","                3,\n","                padding=1,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU'),\n","                inplace=False)\n","            self.lateral_convs.append(l_conv)\n","            self.fpn_convs.append(fpn_conv)\n","\n","        self.fpn_bottleneck = ConvModule(\n","            len(in_channels) * channels,\n","            channels,\n","            3,\n","            padding=1,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'))\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n","\n","    def psp_forward(self, inputs):\n","        \"\"\"Forward function of PSP module.\"\"\"\n","        x = inputs[-1]\n","        psp_outs = [x]\n","        psp_outs.extend(self.psp_modules(x))\n","        psp_outs = torch.cat(psp_outs, dim=1)\n","        output = self.bottleneck(psp_outs)\n","\n","        return output\n","\n","    def _transform_inputs(self, inputs):\n","        \"\"\"Transform inputs for decoder.\n","\n","        Args:\n","            inputs (list[Tensor]): List of multi-level img features.\n","\n","        Returns:\n","            Tensor: The transformed inputs\n","        \"\"\"\n","\n","        if self.input_transform == 'resize_concat':\n","            inputs = [inputs[i] for i in self.in_index]\n","            upsampled_inputs = [\n","                resize(\n","                    input=x,\n","                    size=inputs[0].shape[2:],\n","                    mode='bilinear',\n","                    align_corners=self.align_corners) for x in inputs\n","            ]\n","            inputs = torch.cat(upsampled_inputs, dim=1)\n","        elif self.input_transform == 'multiple_select':\n","            inputs = [inputs[i] for i in self.in_index]\n","        else:\n","            inputs = inputs[self.in_index]\n","\n","        return inputs\n","    \n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        inputs = self._transform_inputs(inputs)\n","\n","        # build laterals\n","        laterals = [\n","            lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)\n","        ]\n","\n","        laterals.append(self.psp_forward(inputs))\n","\n","        # build top-down path\n","        used_backbone_levels = len(laterals)\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            prev_shape = laterals[i - 1].shape[2:]\n","            laterals[i - 1] = laterals[i - 1] + resize(\n","                laterals[i],\n","                size=prev_shape,\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","\n","        # build outputs\n","        fpn_outs = [\n","            self.fpn_convs[i](laterals[i])\n","            for i in range(used_backbone_levels - 1)\n","        ]\n","        # append psp feature\n","        fpn_outs.append(laterals[-1])\n","\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            fpn_outs[i] = resize(\n","                fpn_outs[i],\n","                size=fpn_outs[0].shape[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        fpn_outs = torch.cat(fpn_outs, dim=1)\n","        output = self.fpn_bottleneck(fpn_outs)\n","        output = self.cls_seg(output)\n","        return output\n","    \n","class FCNHead(nn.Module):\n","    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n","\n","    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n","\n","    Args:\n","        num_convs (int): Number of convs in the head. Default: 2.\n","        kernel_size (int): The kernel size for convs in the head. Default: 3.\n","        concat_input (bool): Whether concat the input and output of convs\n","            before classification layer.\n","        dilation (int): The dilation rate for convs in the head. Default: 1.\n","    \"\"\"\n","\n","    def __init__(self,\n","                kernel_size=3,\n","                dilation=1,\n","                in_channels=512,\n","                in_index=2,\n","                channels=256,\n","                num_convs=1,\n","                concat_input=False,\n","                dropout_ratio=0.1,\n","                num_classes=2,\n","                norm_cfg=dict(type='BN', requires_grad=True),\n","                align_corners=False):\n","        assert num_convs >= 0 and dilation > 0 and isinstance(dilation, int)\n","        self.align_corners = align_corners\n","        self.in_index = in_index\n","        self.input_transform = None\n","        self.num_convs = num_convs\n","        self.concat_input = concat_input\n","        self.kernel_size = kernel_size\n","        super(FCNHead, self).__init__()\n","        if num_convs == 0:\n","            assert in_channels == channels\n","\n","        conv_padding = (kernel_size // 2) * dilation\n","        convs = []\n","        convs.append(\n","            ConvModule(\n","                in_channels,\n","                channels,\n","                kernel_size=kernel_size,\n","                padding=conv_padding,\n","                dilation=dilation,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU')))\n","        for i in range(num_convs - 1):\n","            convs.append(\n","                ConvModule(\n","                    channels,\n","                    channels,\n","                    kernel_size=kernel_size,\n","                    padding=conv_padding,\n","                    dilation=dilation,\n","                    conv_cfg=None,\n","                    norm_cfg=norm_cfg,\n","                    act_cfg=dict(type='ReLU')))\n","        if num_convs == 0:\n","            self.convs = nn.Identity()\n","        else:\n","            self.convs = nn.Sequential(*convs)\n","        if self.concat_input:\n","            self.conv_cat = ConvModule(\n","                in_channels + channels,\n","                channels,\n","                kernel_size=kernel_size,\n","                padding=kernel_size // 2,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU'))\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n","\n","    def _transform_inputs(self, inputs):\n","        \"\"\"Transform inputs for decoder.\n","\n","        Args:\n","            inputs (list[Tensor]): List of multi-level img features.\n","\n","        Returns:\n","            Tensor: The transformed inputs\n","        \"\"\"\n","\n","        if self.input_transform == 'resize_concat':\n","            inputs = [inputs[i] for i in self.in_index]\n","            upsampled_inputs = [\n","                resize(\n","                    input=x,\n","                    size=inputs[0].shape[2:],\n","                    mode='bilinear',\n","                    align_corners=self.align_corners) for x in inputs\n","            ]\n","            inputs = torch.cat(upsampled_inputs, dim=1)\n","        elif self.input_transform == 'multiple_select':\n","            inputs = [inputs[i] for i in self.in_index]\n","        else:\n","            inputs = inputs[self.in_index]\n","\n","        return inputs\n","    \n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        x = self._transform_inputs(inputs)\n","        output = self.convs(x)\n","        if self.concat_input:\n","            output = self.conv_cat(torch.cat([x, output], dim=1))\n","        output = self.cls_seg(output)\n","        return output"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Ink_detection(nn.Module):\n","    def __init__(self) -> None:\n","        super(Ink_detection, self).__init__()\n","        self.encoder = ConvNeXt(in_chans=CFG.in_chans,\n","                                depths=[3, 3, 16, 3],\n","                                dims=[128, 256, 512, 1024],\n","                                drop_path_rate=0.1,\n","                                layer_scale_init_value=0.1,\n","                                out_indices=[0, 1, 2, 3])\n","        self.decoder_uper = UPerHead(in_channels=[128, 256, 512, 1024],\n","                                     in_index=[0, 1, 2, 3],\n","                                     pool_scales=(1, 2, 3, 6),\n","                                     channels=512,\n","                                     dropout_ratio=0.1,\n","                                     num_classes=CFG.target_size,\n","                                     norm_cfg=dict(type='BN', requires_grad=True),\n","                                     align_corners=False)\n","        self.decoder_fcn = FCNHead(kernel_size=3,\n","                                   dilation=1,\n","                                   in_channels=512,\n","                                   in_index=2,\n","                                   channels=256,\n","                                   num_convs=1,\n","                                   concat_input=False,\n","                                   dropout_ratio=0.1,\n","                                   num_classes=CFG.target_size,\n","                                   norm_cfg=dict(type='BN', requires_grad=True),\n","                                   align_corners=False)\n","        param = torch.ones(2, requires_grad=True)\n","        self.param = nn.Parameter(param)\n","    def forward(self, x):\n","        feature_list = self.encoder(x)\n","        res_uper = self.decoder_uper(feature_list)\n","        res_fcn = self.decoder_fcn(feature_list)\n","        res_uper = resize(input=res_uper,\n","                          size=(CFG.size, CFG.size),\n","                          mode='bilinear',\n","                          align_corners=False)\n","        res_fcn = resize(input=res_fcn,\n","                          size=(CFG.size, CFG.size),\n","                          mode='bilinear',\n","                          align_corners=False)\n","        res = self.param[0] * res_uper +  self.param[1] * res_fcn\n","        return res       "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:22:04.779128Z","iopub.status.busy":"2023-03-19T07:22:04.778480Z","iopub.status.idle":"2023-03-19T07:22:06.646772Z","shell.execute_reply":"2023-03-19T07:22:06.645661Z","shell.execute_reply.started":"2023-03-19T07:22:04.779088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint not loaded\n"]}],"source":["model = Ink_detection().to(CFG.device)\n","from torchsummary import summary\n","# summary(model, input_size=(CFG.in_chans, 224, 224))\n","# x = torch.ones((16, CFG.in_chans, 224, 224)).float().cuda()\n","# o = model(x)\n","model_name = 'ConvNeXt_Uper'\n","if CFG.pretrained:\n","    try:\n","        checkpoint = torch.load(CFG.checpoint, map_location=CFG.device)\n","        models_dict = model.state_dict()\n","        for model_part in models_dict:\n","            if model_part in checkpoint:\n","                models_dict[model_part] = checkpoint[model_part]\n","        model.load_state_dict(models_dict)\n","        print('Checkpoint loaded')\n","    except:\n","        print('Checkpoint not loaded')\n","        pass"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from warmup_scheduler import GradualWarmupScheduler\n","\n","\n","class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    \"\"\"\n","    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n","    \"\"\"\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(\n","            optimizer, multiplier, total_epoch, after_scheduler)\n","\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [\n","                        base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","def get_scheduler(cfg, optimizer):\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, cfg.epochs, eta_min=1e-7)\n","    scheduler = GradualWarmupSchedulerV2(\n","        optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n","\n","    return scheduler\n","\n","def scheduler_step(scheduler, avg_val_loss, epoch):\n","    scheduler.step(epoch)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train_step(train_loader, model, criterion, optimizer, writer, device, epoch):\n","    model.train()\n","    epoch_loss = 0\n","    scaler = GradScaler(enabled=CFG.use_amp)\n","    bar = tqdm(enumerate(train_loader), total=len(train_loader)) \n","    for step, (image, label, _) in bar:\n","        optimizer.zero_grad()\n","        outputs = model(image.to(device))\n","        # outputs = resize(input=outputs,\n","        #                 size=(CFG.size, CFG.size),\n","        #                 mode='bilinear',\n","        #                 align_corners=False)\n","        loss = criterion(outputs, label.to(device))\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        bar.set_postfix(loss=f'{loss.item():0.4f}', epoch=epoch ,gpu_mem=f'{mem:0.2f} GB', lr=f'{optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:0.2e}')\n","        epoch_loss += loss.item()\n","    writer.add_scalar('Train/Loss', epoch_loss / len(train_loader), epoch)\n","    return epoch_loss / len(train_loader)\n","\n","def valid_step(valid_loader, model, criterion, device, writer, epoch):\n","    pred_label = np.zeros(CFG.shape_list[0])\n","    true_label = CFG.val_label\n","    mask_count = np.zeros(CFG.shape_list[0])\n","    model.eval()\n","    epoch_loss = 0\n","    for step, (images, labels, positions) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            y_preds = model(images)\n","            # y_preds = resize(input=y_preds,\n","            #             size=(CFG.size, CFG.size),\n","            #             mode='bilinear',\n","            #             align_corners=False)\n","            loss = criterion(y_preds, labels)\n","        # make whole mask\n","        y_preds = torch.sigmoid(y_preds)\n","        pred_img =y_preds.squeeze().cpu().numpy()\n","        positions = positions.squeeze()\n","        for i in range(len(positions)):\n","            x1, y1, x2, y2, _ = positions[i].numpy().tolist()\n","            pred_label[y1:y2, x1:x2] += pred_img[i]\n","            mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n","        epoch_loss += loss.item()\n","    avg_loss = epoch_loss / len(valid_loader)\n","    print(f'mask_count_min: {mask_count.min()}')\n","    print(f'mask_count_max: {mask_count.max()}')\n","    # 计算准确率\n","    pred_label /= mask_count\n","    pred_label *= CFG.val_mask\n","    best_th = 0\n","    best_dice = 0\n","    for th in np.arange(3, 6, 0.5) / 10:\n","        dice_score = dice_coef(torch.from_numpy(true_label).to(CFG.device), torch.from_numpy(pred_label).to(CFG.device), thr=th).item()\n","        # dice_scores.append(dice_score)\n","        if dice_score > best_dice:\n","            best_dice = dice_score\n","            best_th = th\n","    if CFG.all_best_dice < best_dice:\n","        print('best_th={:2f}' .format(best_th),\"score up: {:2f}->{:2f}\".format(CFG.all_best_dice, best_dice))       \n","        cv2.imwrite('result/logs/img/'+str(epoch) + '_res.png', ((pred_label > best_th).astype('int')* 255) )\n","        CFG.all_best_dice = best_dice\n","        torch.save(model.state_dict(), 'result/' +  '{}-DIM-{}-[eval_loss]-{:.4f}-[dice_score]-{:.2f}-'.format(model_name, CFG.in_chans , avg_loss, best_dice) + str(epoch) + '-epoch.pkl')\n","        # 使用make_grid将图片转换成网格形式\n","        # pred_mask = make_grid(torch.from_numpy(pred_label ).to(CFG.device),normalize=True)\n","        # true_mask = make_grid(torch.from_numpy(true_label).to(CFG.device), normalize=True)\n","        # 使用add_image方法将图片添加到TensorBoard中\n","        # writer.add_image('Valid/True_mask', true_mask, global_step=epoch, dataformats=\"CHW\")\n","        # writer.add_image('Valid/Pred_mask', pred_mask, global_step=epoch, dataformats=\"CHW\")\n","    if CFG.all_best_loss > avg_loss:\n","        print('best_loss={:2f}'.format(avg_loss), \"loss down: {:2f}->{:2f}\".format(CFG.all_best_loss, avg_loss))\n","        cv2.imwrite('result/logs/img/' + str(epoch) + '_res.png', ((pred_label > best_th).astype('int')* 255) )\n","        CFG.all_best_loss = avg_loss\n","        torch.save(model.state_dict(), 'result/' +  '{}-DIM-{}-[eval_loss]-{:.4f}-[dice_score]-{:.2f}-'.format(model_name, CFG.in_chans ,avg_loss, best_dice) + str(epoch) + '-epoch.pkl')\n","    writer.add_scalar('Val/Dice', best_dice, epoch)\n","    writer.add_scalar('Valid/Loss', avg_loss , epoch)\n","    return avg_loss"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 660/660 [06:32<00:00,  1.68it/s, epoch=1, gpu_mem=7.03 GB, loss=0.3897, lr=1.00e-04]"]},{"name":"stdout","output_type":"stream","text":["val:\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 164/164 [00:33<00:00,  4.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["mask_count_min: 0.0\n","mask_count_max: 4.0\n","best_th=0.300000 score up: 0.000000->0.319557\n","best_loss=0.409941 loss down: inf->0.409941\n","train:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 660/660 [06:39<00:00,  1.65it/s, epoch=2, gpu_mem=7.47 GB, loss=0.4697, lr=1.00e-03]"]},{"name":"stdout","output_type":"stream","text":["val:\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 164/164 [00:33<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["mask_count_min: 0.0\n","mask_count_max: 4.0\n","train:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 660/660 [06:42<00:00,  1.64it/s, epoch=3, gpu_mem=7.47 GB, loss=0.4186, lr=1.00e-03]"]},{"name":"stdout","output_type":"stream","text":["val:\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 164/164 [00:33<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["mask_count_min: 0.0\n","mask_count_max: 4.0\n","train:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 660/660 [06:42<00:00,  1.64it/s, epoch=4, gpu_mem=7.47 GB, loss=0.2907, lr=9.89e-04]"]},{"name":"stdout","output_type":"stream","text":["val:\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 164/164 [00:33<00:00,  4.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["mask_count_min: 0.0\n","mask_count_max: 4.0\n","best_loss=0.407708 loss down: 0.409941->0.407708\n","train:\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 525/660 [05:15<01:18,  1.72it/s, epoch=5, gpu_mem=7.47 GB, loss=0.2150, lr=9.76e-04]"]}],"source":["criterion = smp.losses.SoftBCEWithLogitsLoss()\n","optimizer = optim.AdamW(model.parameters(),\n","                        lr=CFG.lr,\n","                        betas=(0.9, 0.999),\n","                        weight_decay=CFG.weight_decay\n","                        )\n","scheduler = get_scheduler(CFG, optimizer)\n","writer = SummaryWriter('result/logs')\n","for i in range(CFG.epochs):\n","    print('train:')\n","    train_step(train_loader, model, criterion, optimizer, writer, CFG.device, i + 1)\n","    print('val:')\n","    val_loss = valid_step(valid_loader, model, criterion, CFG.device, writer, i + 1)\n","    scheduler_step(scheduler, val_loss, i + 1)"]}],"metadata":{"kernelspec":{"display_name":"dtt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"61166640a95ac08c4dfcc44984648f9045f44ea39ff0c31cc75bc4fde1586daf"}}},"nbformat":4,"nbformat_minor":4}
