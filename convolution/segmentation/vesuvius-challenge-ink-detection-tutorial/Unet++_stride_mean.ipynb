{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff3c9fb9-0c86-4acf-9162-c741c46e53a4","_kg_hide-output":false,"_uuid":"91db1348-a896-4607-8686-f6c6df6419ed","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:30:35.202851Z","iopub.status.busy":"2023-03-19T07:30:35.202431Z","iopub.status.idle":"2023-03-19T07:30:53.178442Z","shell.execute_reply":"2023-03-19T07:30:53.177132Z","shell.execute_reply.started":"2023-03-19T07:30:35.202813Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","from torch.utils.data import ConcatDataset\n","import os\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import warnings\n","import gc\n","import cv2\n","# 忽略所有警告\n","warnings.filterwarnings('ignore')\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","seed_value = 42   # 设定随机数种子\n","\n","np.random.seed(seed_value)\n","random.seed(seed_value)\n","os.environ['PYTHONHASHSEED'] = str(seed_value)  # 为了禁止hash随机化，使得实验可复现。\n","\n","torch.manual_seed(seed_value)     # 为CPU设置随机种子\n","torch.cuda.manual_seed(seed_value)      # 为当前GPU设置随机种子（只用一块GPU）\n","torch.cuda.manual_seed_all(seed_value)   # 为所有GPU设置随机种子（多块GPU）\n","\n","torch.backends.cudnn.deterministic = True\n","\n","class CFG:\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    checpoint = ''\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","\n","    # # comp_dir_path = './'\n","    # comp_dir_path = '/kaggle/input/'\n","    # comp_folder_name = 'vesuvius-challenge-ink-detection'\n","    # # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    # comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","        # comp_dir_path = './'\n","    comp_dir_path = ''\n","    comp_folder_name = 'data'\n","    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","    \n","    exp_name = 'Unet++_stride'\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'Unet++'\n","\n","    in_chans = 64# 65\n","    # ============== training cfg =============\n","    size = 224\n","    tile_size = 224\n","    stride = tile_size // 2\n","    mean_num = 4\n","\n","    train_batch_size = 16 # 32\n","    valid_batch_size = 16\n","    use_amp = True\n","\n","    epochs = 50 # 30\n","\n","    # lr = 1e-4 / warmup_factor\n","    lr = 1e-5\n","\n","    # ============== fixed =============\n","    pretrained = False\n","\n","    backbone = 'se_resnext50_32x4d'\n","\n","    min_lr = 1e-6\n","    weight_decay = 1e-6\n","    max_grad_norm = 1000\n","\n","    num_workers = 4\n","\n","    seed = 42\n","\n","    threshhold = 0.5\n","\n","    shape_list = []\n","    test_shape_list = []\n","\n","    # ============== set dataset path =============\n","\n","    # outputs_path = f'/kaggle/working/outputs/{comp_name}/{exp_name}/'\n","    outputs_path = 'result/'\n","\n","    submission_dir = outputs_path + 'submissions/'\n","    submission_path = submission_dir + f'submission_{exp_name}.csv'\n","\n","    model_dir = outputs_path\n","    log_dir = outputs_path + 'logs/'\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        # A.RandomResizedCrop(\n","        #     size, size, scale=(0.85, 1.0)),\n","        A.Resize(size, size),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.75),\n","        A.ShiftScaleRotate(p=0.75),\n","        A.OneOf([\n","                A.GaussNoise(var_limit=[10, 50]),\n","                A.GaussianBlur(),\n","                A.MotionBlur(),\n","                ], p=0.4),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                        mask_fill_value=0, p=0.5),\n","        # A.Cutout(max_h_size=int(size * 0.6),\n","        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n","        A.Normalize(\n","            mean= [0] * (in_chans // mean_num),\n","            std= [1] * (in_chans // mean_num)\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","\n","    valid_aug_list = [\n","        A.Resize(size, size),\n","        A.Normalize(\n","            mean= [0] * (in_chans // mean_num),\n","            std= [1] * (in_chans // mean_num)\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_image_mask(fragment_id):\n","\n","    images = []\n","\n","    # idxs = range(65)\n","    mid = 65 // 2\n","    start = mid - CFG.in_chans // 2\n","    end = mid + CFG.in_chans // 2\n","    idxs = range(start, end, CFG.mean_num)\n","\n","    for i in tqdm(idxs):\n","        flag = False\n","        for j in range(i, i + CFG.mean_num):\n","            image = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/surface_volume/{j:02}.tif\", 0).astype(np.float32)\n","\n","            pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n","            pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n","\n","            image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","            if not flag:\n","                _image = np.zeros_like(image)\n","                flag = True\n","            _image += image\n","        \n","        images.append(_image / CFG.mean_num)\n","        del _image\n","        del image\n","        gc.collect()\n","    images = np.stack(images, axis=2)\n","\n","    label = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/inklabels.png\", 0)\n","    label = np.pad(label, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    label = label.astype('float32')\n","    label /= 255.0\n","\n","    mask = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/mask.png\", 0)\n","    mask = np.pad(mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    mask = mask.astype('float32')\n","    mask /= 255.0\n","\n","    CFG.shape_list.append(mask.shape)\n","    \n","    return images, label, mask\n","\n","def get_train_valid_dataset(val_persent=0.05):\n","    images = []\n","    labels = []\n","    positions = []\n","    indexs = []\n","    train_images = []\n","    train_labels = []\n","\n","    valid_images = []\n","    valid_labels = []\n","    valid_positons = []\n","\n","    index = 0\n","\n","    for fragment_id in range(1, 4):\n","\n","        image, label, mask = read_image_mask(fragment_id)\n","\n","        x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n","        y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n","\n","        for y1 in y1_list:\n","            for x1 in x1_list:\n","                y2 = y1 + CFG.tile_size\n","                x2 = x1 + CFG.tile_size\n","                if sum(list(mask[y1:y2, x1:x2].flatten())) != 0:\n","                    images.append(image[y1:y2, x1:x2])\n","                    labels.append(label[y1:y2, x1:x2, None])\n","                    positions.append([x1, y1, x2, y2, fragment_id - 1])\n","                    indexs.append(index)\n","                    index += 1\n","    random.shuffle(indexs)\n","    n = int(val_persent * len(indexs))\n","    train_indexs = indexs[:-n]\n","    for i, (image, label, position) in enumerate(zip(images, labels, positions)):\n","        if i in train_indexs:\n","            train_images.append(image)\n","            train_labels.append(label)\n","        else:\n","            valid_images.append(image)\n","            valid_labels.append(label)\n","            valid_positons.append(position)\n","    return train_images, train_labels, valid_images, valid_labels, valid_positons\n","\n","def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","    return aug\n","\n","class SubvolumeDataset(data.Dataset):\n","    def __init__(self, images, labels, positions, transform, is_train):\n","        self.transform = transform\n","        self.images = images\n","        self.labels = labels\n","        self.is_train = is_train\n","        self.positions = positions\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, index):\n","        if self.is_train:\n","            image = self.images[index]\n","            label = self.labels[index]\n","            if self.positions:\n","                position = np.array(self.positions[index])\n","            else:\n","                position = np.zeros(1)\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","                label = data['mask']\n","            return image, label, position\n","        else:\n","            image = self.images[index]\n","            position = np.array(self.positions[index])\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","            return image, position\n","        \n","# IOU and Dice Score\n","def dice_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2 * inter + epsilon) / (den + epsilon)).mean()\n","    return dice\n","\n","\n","def iou_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    union = (y_true + y_pred - y_true * y_pred).sum(dim=dim)\n","    iou = ((inter + epsilon) / (union + epsilon)).mean()\n","    return iou"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_images, train_labels, valid_images, valid_labels, valid_positons = get_train_valid_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = SubvolumeDataset(train_images, train_labels, None,get_transforms(data='train', cfg=CFG), True)\n","valid_dataset = SubvolumeDataset(valid_images, valid_labels, valid_positons, get_transforms(data='valid', cfg=CFG), True)\n","train_loader = data.DataLoader(train_dataset,\n","                          batch_size=CFG.train_batch_size,\n","                          shuffle=True,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n","                          )\n","valid_loader = data.DataLoader(valid_dataset,\n","                          batch_size=CFG.valid_batch_size,\n","                          shuffle=False,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:22:04.779128Z","iopub.status.busy":"2023-03-19T07:22:04.778480Z","iopub.status.idle":"2023-03-19T07:22:06.646772Z","shell.execute_reply":"2023-03-19T07:22:06.645661Z","shell.execute_reply.started":"2023-03-19T07:22:04.779088Z"},"trusted":true},"outputs":[],"source":["# model = Ringed_Res_Unet(n_channels=CFG.in_chans, n_classes=CFG.target_size).to(CFG.device)\n","model = smp.UnetPlusPlus(in_channels=CFG.in_chans // CFG.mean_num, classes=1, encoder_name=CFG.backbone).to(CFG.device)\n","# x = np.zeros((16, CFG.in_chans, 224, 224))\n","# x = torch.from_numpy(x).to(CFG.device).float()\n","# out = model(x)\n","# print(out.shape)\n","model_name = 'UnetPlusPlus'\n","if CFG.pretrained:\n","    try:\n","        checkpoint = torch.load(CFG.checpoint, map_location=CFG.device)\n","        models_dict = model.state_dict()\n","        for model_part in models_dict:\n","            if model_part in checkpoint:\n","                models_dict[model_part] = checkpoint[model_part]\n","        model.load_state_dict(models_dict)\n","        print('Checkpoint loaded')\n","    except:\n","        print('Checkpoint not loaded')\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_step(train_loader, model, criterion, optimizer, writer, device, epoch):\n","    model.train()\n","    epoch_loss = 0\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * 10, eta_min=CFG.min_lr)\n","    bar = tqdm(enumerate(train_loader), total=len(train_loader)) \n","    for step, (image, label, _) in bar:\n","        optimizer.zero_grad()\n","        outputs = model(image.to(device))\n","        loss = criterion(outputs, label.to(device))\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        bar.set_postfix(loss=f'{loss.item():0.4f}', epoch=epoch ,gpu_mem=f'{mem:0.2f} GB', lr=f'{optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:0.2e}')\n","        epoch_loss += loss.item()\n","    writer.add_scalar('Train/Loss', epoch_loss / len(train_loader), epoch)\n","\n","def valid_step(valid_loader, model, criterion, device, writer, epoch):\n","    pred_label_list = []\n","    label_list = []\n","    for i in range(3):\n","        pred_label_list.append(torch.zeros(CFG.shape_list[i]))\n","        label_list.append(torch.zeros(CFG.shape_list[i]))\n","    model.eval()\n","    epoch_loss = 0\n","    dice_scores = []\n","    iou_scores = []\n","\n","    for step, (images, labels, positions) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            y_preds = model(images)\n","            loss = criterion(y_preds, labels)\n","        # make whole mask\n","        y_preds = torch.sigmoid(y_preds)\n","        pred_img = (y_preds > CFG.threshhold).float()\n","        positions = positions.squeeze()\n","        for i in range(len(positions)):\n","            x1, y1, x2, y2, fragment_id = positions[i].numpy().tolist()\n","            pred_label_list[fragment_id][y1:y2, x1:x2] = pred_img[i].squeeze(0).cpu()\n","            label_list[fragment_id][y1:y2, x1:x2] = labels[i].squeeze(0).cpu()\n","        epoch_loss += loss.item()\n","        # 计算准确率\n","        dice_score = dice_coef(labels.to(device), y_preds.to(device), thr=CFG.threshhold).item()\n","        iou_socre = iou_coef(labels.to(device), y_preds.to(device), thr=CFG.threshhold).item()\n","\n","        dice_scores.append(dice_score)\n","        iou_scores.append(iou_socre)\n","                \n","    # 使用make_grid将图片转换成网格形式\n","    for i in range(len(pred_label_list)):\n","\n","        pred_mask = make_grid(pred_label_list[i], normalize=True)\n","        true_mask = make_grid(label_list[i], normalize=True)\n","        # 使用add_image方法将图片添加到TensorBoard中\n","        writer.add_image('Valid/' + str(i) + '_True_mask', true_mask, global_step=epoch, dataformats=\"CHW\")\n","        writer.add_image('Valid/' + str(i) + '_Pred_mask', pred_mask, global_step=epoch, dataformats=\"CHW\")\n","\n","    writer.add_scalar('Val/IOU', sum(iou_scores) / len(iou_scores), epoch)\n","    writer.add_scalar('Val/Dice', sum(dice_scores) / len(dice_scores), epoch)\n","    writer.add_scalar('Valid/Loss', epoch_loss / len(valid_loader), epoch)\n","    torch.save(model.state_dict(), 'result/' +  '{}-DIM-{}-[eval_loss]-{:.4f}-[dice_score]-{:.2f}-[iou_score]-{:.2f}-'.format(model_name, CFG.in_chans , epoch_loss / len(valid_loader), sum(dice_scores) / len(dice_scores), sum(iou_scores) / len(iou_scores)) + str(epoch) + '-epoch.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = smp.losses.SoftBCEWithLogitsLoss()\n","optimizer = optim.AdamW(model.parameters(),\n","                        lr=CFG.lr,\n","                        betas=(0.9, 0.999),\n","                        weight_decay=CFG.weight_decay\n","                        )\n","writer = SummaryWriter('result/logs')\n","for i in range(CFG.epochs):\n","    print('train:')\n","    train_step(train_loader, model, criterion, optimizer, writer, CFG.device, i + 1)\n","    print('val:')\n","    valid_step(valid_loader, model, criterion, CFG.device, writer, i + 1)"]}],"metadata":{"kernelspec":{"display_name":"dtt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"61166640a95ac08c4dfcc44984648f9045f44ea39ff0c31cc75bc4fde1586daf"}}},"nbformat":4,"nbformat_minor":4}
