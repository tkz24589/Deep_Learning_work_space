{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"ff3c9fb9-0c86-4acf-9162-c741c46e53a4","_kg_hide-output":false,"_uuid":"91db1348-a896-4607-8686-f6c6df6419ed","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:30:35.202851Z","iopub.status.busy":"2023-03-19T07:30:35.202431Z","iopub.status.idle":"2023-03-19T07:30:53.178442Z","shell.execute_reply":"2023-03-19T07:30:53.177132Z","shell.execute_reply.started":"2023-03-19T07:30:35.202813Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","from torch.utils.data import ConcatDataset\n","import os\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import warnings\n","import gc\n","import cv2\n","# 忽略所有警告\n","warnings.filterwarnings('ignore')\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","seed_value = 42   # 设定随机数种子\n","\n","np.random.seed(seed_value)\n","random.seed(seed_value)\n","os.environ['PYTHONHASHSEED'] = str(seed_value)  # 为了禁止hash随机化，使得实验可复现。\n","\n","torch.manual_seed(seed_value)     # 为CPU设置随机种子\n","torch.cuda.manual_seed(seed_value)      # 为当前GPU设置随机种子（只用一块GPU）\n","torch.cuda.manual_seed_all(seed_value)   # 为所有GPU设置随机种子（多块GPU）\n","\n","torch.backends.cudnn.deterministic = True\n","\n","class CFG:\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    checpoint = ''\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","\n","    # # comp_dir_path = './'\n","    # comp_dir_path = '/kaggle/input/'\n","    # comp_folder_name = 'vesuvius-challenge-ink-detection'\n","    # # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    # comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","        # comp_dir_path = './'\n","    comp_dir_path = ''\n","    comp_folder_name = 'data'\n","    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","    \n","    exp_name = 'ConvNeXt_UPerHead'\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'ConvNeXt_UPerHead'\n","\n","    in_chans = 16 # 65\n","    # ============== training cfg =============\n","    size = 224\n","    tile_size = 224\n","    stride = tile_size // 2\n","\n","    mean_num = 4\n","\n","    train_batch_size = 16 # 32\n","    valid_batch_size = 16\n","    use_amp = True\n","\n","    epochs = 50 # 30\n","\n","    # lr = 1e-4 / warmup_factor\n","    lr = 1e-2\n","\n","    # ============== fixed =============\n","    pretrained = False\n","\n","    min_lr = 1e-6\n","    weight_decay = 1e-6\n","    max_grad_norm = 1000\n","\n","    num_workers = 4\n","\n","    seed = 42\n","\n","    threshhold = 0.4\n","\n","    shape_list = []\n","    test_shape_list = []\n","\n","    # ============== set dataset path =============\n","\n","    # outputs_path = f'/kaggle/working/outputs/{comp_name}/{exp_name}/'\n","    outputs_path = 'result/'\n","\n","    submission_dir = outputs_path + 'submissions/'\n","    submission_path = submission_dir + f'submission_{exp_name}.csv'\n","\n","    model_dir = outputs_path\n","    log_dir = outputs_path + 'logs/'\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        # A.RandomResizedCrop(\n","        #     size, size, scale=(0.85, 1.0)),\n","        A.Resize(size, size),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.75),\n","        A.ShiftScaleRotate(p=0.75),\n","        A.OneOf([\n","                A.GaussNoise(var_limit=[10, 50]),\n","                A.GaussianBlur(),\n","                A.MotionBlur(),\n","                ], p=0.4),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                        mask_fill_value=0, p=0.5),\n","        # A.Cutout(max_h_size=int(size * 0.6),\n","        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n","        A.Normalize(\n","            mean= [0] * (in_chans // mean_num),\n","            std= [1] * (in_chans // mean_num)\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","\n","    valid_aug_list = [\n","        A.Resize(size, size),\n","        A.Normalize(\n","            mean= [0] * (in_chans // mean_num),\n","            std= [1] * (in_chans // mean_num)\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_image_mask(fragment_id):\n","\n","    images = []\n","\n","    # idxs = range(65)\n","    mid = 65 // 2\n","    start = mid - CFG.in_chans // 2\n","    end = mid + CFG.in_chans // 2\n","    idxs = range(start, end, CFG.mean_num)\n","\n","    for i in tqdm(idxs):\n","        flag = False\n","        for j in range(i, i + CFG.mean_num):\n","            image = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/surface_volume/{j:02}.tif\", 0).astype(np.float32)\n","\n","            pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n","            pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n","\n","            image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","            if not flag:\n","                _image = image.copy()\n","                flag = True\n","            else:\n","                _image += image\n","        images.append(_image / CFG.mean_num)\n","    images = np.stack(images, axis=2)\n","\n","    label = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/inklabels.png\", 0)\n","    label = np.pad(label, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    label = label.astype('float32')\n","    label /= 255.0\n","\n","    mask = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/mask.png\", 0)\n","    mask = np.pad(mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    mask = mask.astype('float32')\n","    mask /= 255.0\n","\n","    CFG.shape_list.append(mask.shape)\n","    \n","    return images, label, mask\n","\n","def get_train_valid_dataset(val_persent=0.05):\n","    images = []\n","    labels = []\n","    positions = []\n","    indexs = []\n","    train_images = []\n","    train_labels = []\n","\n","    valid_images = []\n","    valid_labels = []\n","    valid_positons = []\n","\n","    index = 0\n","\n","    for fragment_id in range(1, 4):\n","\n","        image, label, mask = read_image_mask(fragment_id)\n","\n","        x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n","        y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n","\n","        for y1 in y1_list:\n","            for x1 in x1_list:\n","                y2 = y1 + CFG.tile_size\n","                x2 = x1 + CFG.tile_size\n","                if sum(list(mask[y1:y2, x1:x2].flatten())) != 0:\n","                    images.append(image[y1:y2, x1:x2])\n","                    labels.append(label[y1:y2, x1:x2, None])\n","                    positions.append([x1, y1, x2, y2, fragment_id - 1])\n","                    indexs.append(index)\n","                    index += 1\n","    random.shuffle(indexs)\n","    n = int(val_persent * len(indexs))\n","    train_indexs = indexs[:-n]\n","    for i, (image, label, position) in enumerate(zip(images, labels, positions)):\n","        if i in train_indexs:\n","            train_images.append(image)\n","            train_labels.append(label)\n","        else:\n","            valid_images.append(image)\n","            valid_labels.append(label)\n","            valid_positons.append(position)\n","    return train_images, train_labels, valid_images, valid_labels, valid_positons"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","    return aug\n","\n","class SubvolumeDataset(data.Dataset):\n","    def __init__(self, images, labels, positions, transform, is_train):\n","        self.transform = transform\n","        self.images = images\n","        self.labels = labels\n","        self.is_train = is_train\n","        self.positions = positions\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, index):\n","        if self.is_train:\n","            image = self.images[index]\n","            label = self.labels[index]\n","            if self.positions:\n","                position = np.array(self.positions[index])\n","            else:\n","                position = np.zeros(1)\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","                label = data['mask']\n","            return image, label, position\n","        else:\n","            image = self.images[index]\n","            position = np.array(self.positions[index])\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","            return image, position\n","        \n","# IOU and Dice Score\n","def dice_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2 * inter + epsilon) / (den + epsilon)).mean()\n","    return dice\n","\n","\n","def iou_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    union = (y_true + y_pred - y_true * y_pred).sum(dim=dim)\n","    iou = ((inter + epsilon) / (union + epsilon)).mean()\n","    return iou"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:22<00:00,  5.53s/it]\n","100%|██████████| 4/4 [00:30<00:00,  7.73s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["train_images, train_labels, valid_images, valid_labels, valid_positons = get_train_valid_dataset()\n","print()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_dataset = SubvolumeDataset(train_images, train_labels, None,get_transforms(data='train', cfg=CFG), True)\n","valid_dataset = SubvolumeDataset(valid_images, valid_labels, valid_positons, get_transforms(data='valid', cfg=CFG), True)\n","train_loader = data.DataLoader(train_dataset,\n","                          batch_size=CFG.train_batch_size,\n","                          shuffle=True,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n","                          )\n","valid_loader = data.DataLoader(valid_dataset,\n","                          batch_size=CFG.valid_batch_size,\n","                          shuffle=False,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from functools import partial\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import trunc_normal_, DropPath\n","from mmcv.utils import get_logger\n","import logging\n","from mmcv.cnn import ConvModule\n","\n","def resize(input,\n","           size=None,\n","           scale_factor=None,\n","           mode='nearest',\n","           align_corners=None,\n","           warning=True):\n","    if warning:\n","        if size is not None and align_corners:\n","            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n","            output_h, output_w = tuple(int(x) for x in size)\n","            if output_h > input_h or output_w > output_h:\n","                if ((output_h > 1 and output_w > 1 and input_h > 1\n","                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n","                        and (output_w - 1) % (input_w - 1)):\n","                    warnings.warn(\n","                        f'When align_corners={align_corners}, '\n","                        'the output would more aligned if '\n","                        f'input size {(input_h, input_w)} is `x+1` and '\n","                        f'out size {(output_h, output_w)} is `nx+1`')\n","    return F.interpolate(input, size, scale_factor, mode, align_corners)\n","\n","def get_root_logger(log_file=None, log_level=logging.INFO):\n","    \"\"\"Get the root logger.\n","\n","    The logger will be initialized if it has not been initialized. By default a\n","    StreamHandler will be added. If `log_file` is specified, a FileHandler will\n","    also be added. The name of the root logger is the top-level package name,\n","    e.g., \"mmseg\".\n","\n","    Args:\n","        log_file (str | None): The log filename. If specified, a FileHandler\n","            will be added to the root logger.\n","        log_level (int): The root logger level. Note that only the process of\n","            rank 0 is affected, while other processes will set the level to\n","            \"Error\" and be silent most of the time.\n","\n","    Returns:\n","        logging.Logger: The root logger.\n","    \"\"\"\n","\n","    logger = get_logger(name='mmseg', log_file=log_file, log_level=log_level)\n","\n","    return logger\n","\n","class BayarConv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding=2):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.minus1 = (torch.ones(self.in_channels, self.out_channels, 1) * -1.000)\n","\n","        super(BayarConv2d, self).__init__()\n","        # only (kernel_size ** 2 - 1) trainable params as the center element is always -1\n","        self.kernel = nn.Parameter(torch.rand(self.in_channels, self.out_channels, kernel_size ** 2 - 1),\n","                                   requires_grad=True)\n","\n","\n","    def bayarConstraint(self):\n","        self.kernel.data = self.kernel.permute(2, 0, 1)\n","        self.kernel.data = torch.div(self.kernel.data, self.kernel.data.sum(0))\n","        self.kernel.data = self.kernel.permute(1, 2, 0)\n","        ctr = self.kernel_size ** 2 // 2\n","        real_kernel = torch.cat((self.kernel[:, :, :ctr], self.minus1.to(self.kernel.device), self.kernel[:, :, ctr:]), dim=2)\n","        real_kernel = real_kernel.reshape((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n","        return real_kernel\n","\n","    def forward(self, x):\n","        x = F.conv2d(x, self.bayarConstraint(), stride=self.stride, padding=self.padding)\n","        return x\n","\n","class Block(nn.Module):\n","    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n","    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n","    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n","    We use (2) as we find it slightly faster in PyTorch\n","    \n","    Args:\n","        dim (int): Number of input channels.\n","        drop_path (float): Stochastic depth rate. Default: 0.0\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","    \"\"\"\n","    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n","        super().__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n","        self.norm = LayerNorm(dim, eps=1e-6)\n","        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n","        self.act = nn.GELU()\n","        self.pwconv2 = nn.Linear(4 * dim, dim)\n","        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n","                                    requires_grad=True) if layer_scale_init_value > 0 else None\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.dwconv(x)\n","        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n","        x = self.norm(x)\n","        x = self.pwconv1(x)\n","        x = self.act(x)\n","        x = self.pwconv2(x)\n","        if self.gamma is not None:\n","            x = self.gamma * x\n","        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n","\n","        x = input + self.drop_path(x)\n","        return x\n","\n","class LayerNorm(nn.Module):\n","    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n","    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n","    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n","    with shape (batch_size, channels, height, width).\n","    \"\"\"\n","    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.data_format = data_format\n","        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n","            raise NotImplementedError \n","        self.normalized_shape = (normalized_shape, )\n","    \n","    def forward(self, x):\n","        if self.data_format == \"channels_last\":\n","            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.data_format == \"channels_first\":\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x\n","\n","class ConvNeXt(nn.Module):\n","    r\"\"\" ConvNeXt\n","        A PyTorch impl of : `A ConvNet for the 2020s`  -\n","          https://arxiv.org/pdf/2201.03545.pdf\n","\n","    Args:\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n","        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n","    \"\"\"\n","    def __init__(self, in_chans=3, bayar=False, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], \n","                 drop_path_rate=0., layer_scale_init_value=1e-6, out_indices=[0, 1, 2, 3], pretrained=None\n","                 ):\n","        super().__init__()\n","        self.pretrained = pretrained\n","        self.in_chans = in_chans\n","        self.bayar = bayar\n","        if self.bayar:\n","            self.bayar_conv = BayarConv2d(in_chans, in_chans)\n","            self.in_chans = in_chans * 2\n","\n","        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n","        stem = nn.Sequential(\n","            nn.Conv2d(self.in_chans, dims[0], kernel_size=4, stride=4),\n","            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n","        )\n","        self.downsample_layers.append(stem)\n","        for i in range(len(dims) - 1):\n","            downsample_layer = nn.Sequential(\n","                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n","                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n","        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n","        cur = 0\n","        for i in range(len(dims)):\n","            stage = nn.Sequential(\n","                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n","                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n","            )\n","            self.stages.append(stage)\n","            cur += depths[i]\n","\n","        self.out_indices = out_indices\n","\n","        norm_layer = partial(LayerNorm, eps=1e-6, data_format=\"channels_first\")\n","        for i_layer in range(len(dims)):\n","            layer = norm_layer(dims[i_layer])\n","            layer_name = f'norm{i_layer}'\n","            self.add_module(layer_name, layer)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, (nn.Conv2d, nn.Linear)):\n","            trunc_normal_(m.weight, std=.02)\n","            nn.init.constant_(m.bias, 0)\n","\n","    def init_weights(self, pretrained=None):\n","        \"\"\"Initialize the weights in backbone.\n","        Args:\n","            pretrained (str, optional): Path to pre-trained weights.\n","                Defaults to None.\n","        \"\"\"\n","        if pretrained is None:\n","            pretrained = self.pretrained\n","\n","        def _init_weights(m):\n","            if isinstance(m, nn.Linear):\n","                trunc_normal_(m.weight, std=.02)\n","                if isinstance(m, nn.Linear) and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.LayerNorm):\n","                nn.init.constant_(m.bias, 0)\n","                nn.init.constant_(m.weight, 1.0)\n","\n","        if isinstance(pretrained, str):\n","            self.apply(_init_weights)\n","            logger = get_root_logger()\n","        elif pretrained is None:\n","            self.apply(_init_weights)\n","        else:\n","            raise TypeError('pretrained must be a str or None')\n","\n","    def forward_features(self, x):\n","        if self.bayar:\n","            x_bayar = self.bayar_conv(x)\n","            x = torch.cat([x, x_bayar], dim=1)\n","        outs = []\n","        for i in range(4):\n","            x = self.downsample_layers[i](x)\n","            x = self.stages[i](x)\n","            if i in self.out_indices:\n","                norm_layer = getattr(self, f'norm{i}')\n","                x_out = norm_layer(x)\n","                outs.append(x_out)\n","\n","        return tuple(outs)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        return x\n","    \n","class PPM(nn.ModuleList):\n","    \"\"\"Pooling Pyramid Module used in PSPNet.\n","\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module.\n","        in_channels (int): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        conv_cfg (dict|None): Config of conv layers.\n","        norm_cfg (dict|None): Config of norm layers.\n","        act_cfg (dict): Config of activation layers.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","    \"\"\"\n","\n","    def __init__(self, pool_scales, in_channels, channels, conv_cfg, norm_cfg,\n","                 act_cfg, align_corners, **kwargs):\n","        super(PPM, self).__init__()\n","        self.pool_scales = pool_scales\n","        self.align_corners = align_corners\n","        self.in_channels = in_channels\n","        self.channels = channels\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        for pool_scale in pool_scales:\n","            self.append(\n","                nn.Sequential(\n","                    nn.AdaptiveAvgPool2d(pool_scale),\n","                    ConvModule(\n","                        self.in_channels,\n","                        self.channels,\n","                        1,\n","                        conv_cfg=self.conv_cfg,\n","                        norm_cfg=self.norm_cfg,\n","                        act_cfg=self.act_cfg,\n","                        **kwargs)))\n","\n","    def forward(self, x):\n","        \"\"\"Forward function.\"\"\"\n","        ppm_outs = []\n","        for ppm in self:\n","            ppm_out = ppm(x)\n","            upsampled_ppm_out = resize(\n","                ppm_out,\n","                size=x.size()[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","            ppm_outs.append(upsampled_ppm_out)\n","        return ppm_outs\n","\n","class UPerHead(nn.Module):\n","    \"\"\"Unified Perceptual Parsing for Scene Understanding.\n","\n","    This head is the implementation of `UPerNet\n","    <https://arxiv.org/abs/1807.10221>`_.\n","\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module applied on the last feature. Default: (1, 2, 3, 6).\n","    \"\"\"\n","\n","    def __init__(self, \n","                in_channels=[128, 256, 512, 1024],\n","                in_index=[0, 1, 2, 3],\n","                pool_scales=(1, 2, 3, 6),\n","                channels=512,\n","                dropout_ratio=0.1,\n","                num_classes=0,\n","                norm_cfg=dict(type='BN', requires_grad=True),\n","                align_corners=False):\n","        super(UPerHead, self).__init__()\n","        # PSP Module\n","        self.input_transform = 'multiple_select'\n","        self.align_corners = align_corners\n","        self.in_index = in_index\n","        self.psp_modules = PPM(\n","            pool_scales,\n","            in_channels[-1],\n","            channels,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'),\n","            align_corners=align_corners)\n","        self.bottleneck = ConvModule(\n","            in_channels[-1] + len(pool_scales) * channels,\n","            channels,\n","            3,\n","            padding=1,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'))\n","        # FPN Module\n","        self.lateral_convs = nn.ModuleList()\n","        self.fpn_convs = nn.ModuleList()\n","        for in_channel in in_channels[:-1]:  # skip the top layer\n","            l_conv = ConvModule(\n","                in_channel,\n","                channels,\n","                1,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU'),\n","                inplace=False)\n","            fpn_conv = ConvModule(\n","                channels,\n","                channels,\n","                3,\n","                padding=1,\n","                conv_cfg=None,\n","                norm_cfg=norm_cfg,\n","                act_cfg=dict(type='ReLU'),\n","                inplace=False)\n","            self.lateral_convs.append(l_conv)\n","            self.fpn_convs.append(fpn_conv)\n","\n","        self.fpn_bottleneck = ConvModule(\n","            len(in_channels) * channels,\n","            channels,\n","            3,\n","            padding=1,\n","            conv_cfg=None,\n","            norm_cfg=norm_cfg,\n","            act_cfg=dict(type='ReLU'))\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n","\n","    def psp_forward(self, inputs):\n","        \"\"\"Forward function of PSP module.\"\"\"\n","        x = inputs[-1]\n","        psp_outs = [x]\n","        psp_outs.extend(self.psp_modules(x))\n","        psp_outs = torch.cat(psp_outs, dim=1)\n","        output = self.bottleneck(psp_outs)\n","\n","        return output\n","\n","    def _transform_inputs(self, inputs):\n","        \"\"\"Transform inputs for decoder.\n","\n","        Args:\n","            inputs (list[Tensor]): List of multi-level img features.\n","\n","        Returns:\n","            Tensor: The transformed inputs\n","        \"\"\"\n","\n","        if self.input_transform == 'resize_concat':\n","            inputs = [inputs[i] for i in self.in_index]\n","            upsampled_inputs = [\n","                resize(\n","                    input=x,\n","                    size=inputs[0].shape[2:],\n","                    mode='bilinear',\n","                    align_corners=self.align_corners) for x in inputs\n","            ]\n","            inputs = torch.cat(upsampled_inputs, dim=1)\n","        elif self.input_transform == 'multiple_select':\n","            inputs = [inputs[i] for i in self.in_index]\n","        else:\n","            inputs = inputs[self.in_index]\n","\n","        return inputs\n","    \n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        inputs = self._transform_inputs(inputs)\n","\n","        # build laterals\n","        laterals = [\n","            lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)\n","        ]\n","\n","        laterals.append(self.psp_forward(inputs))\n","\n","        # build top-down path\n","        used_backbone_levels = len(laterals)\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            prev_shape = laterals[i - 1].shape[2:]\n","            laterals[i - 1] = laterals[i - 1] + resize(\n","                laterals[i],\n","                size=prev_shape,\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","\n","        # build outputs\n","        fpn_outs = [\n","            self.fpn_convs[i](laterals[i])\n","            for i in range(used_backbone_levels - 1)\n","        ]\n","        # append psp feature\n","        fpn_outs.append(laterals[-1])\n","\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            fpn_outs[i] = resize(\n","                fpn_outs[i],\n","                size=fpn_outs[0].shape[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        fpn_outs = torch.cat(fpn_outs, dim=1)\n","        output = self.fpn_bottleneck(fpn_outs)\n","        output = self.cls_seg(output)\n","        return output"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = nn.Sequential(ConvNeXt(in_chans=CFG.in_chans // CFG.mean_num,\n","                                     depths=[3, 3, 27, 3],\n","                                     dims=[128, 256, 512, 1024],\n","                                     drop_path_rate=0.4,\n","                                     layer_scale_init_value=1.0,\n","                                     out_indices=[0, 1, 2, 3]),\n","                            UPerHead(in_channels=[128, 256, 512, 1024],\n","                                     in_index=[0, 1, 2, 3],\n","                                     pool_scales=(1, 2, 3, 6),\n","                                     channels=512,\n","                                     dropout_ratio=0.1,\n","                                     num_classes=CFG.target_size,\n","                                     norm_cfg=dict(type='BN', requires_grad=True),\n","                                     align_corners=False)\n","                            ).to(CFG.device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:22:04.779128Z","iopub.status.busy":"2023-03-19T07:22:04.778480Z","iopub.status.idle":"2023-03-19T07:22:06.646772Z","shell.execute_reply":"2023-03-19T07:22:06.645661Z","shell.execute_reply.started":"2023-03-19T07:22:04.779088Z"},"trusted":true},"outputs":[],"source":["model_name = CFG.model_name\n","if CFG.pretrained:\n","    try:\n","        checkpoint = torch.load(CFG.checpoint, map_location=CFG.device)\n","        models_dict = model.state_dict()\n","        for model_part in models_dict:\n","            if model_part in checkpoint:\n","                models_dict[model_part] = checkpoint[model_part]\n","        model.load_state_dict(models_dict)\n","        print('Checkpoint loaded')\n","    except:\n","        print('Checkpoint not loaded')\n","        pass"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train_step(train_loader, model, criterion, optimizer, writer, device, epoch):\n","    model.train()\n","    epoch_loss = 0\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * 10, eta_min=CFG.min_lr)\n","    bar = tqdm(enumerate(train_loader), total=len(train_loader)) \n","    for step, (image, label, _) in bar:\n","        optimizer.zero_grad()\n","        outputs = model(image.to(device))\n","        outputs = resize(input=outputs,\n","                            size=label.shape[2:],\n","                            mode='bilinear',\n","                            align_corners=False)\n","        loss = criterion(outputs, label.to(device))\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        bar.set_postfix(loss=f'{loss.item():0.4f}', epoch=epoch ,gpu_mem=f'{mem:0.2f} GB', lr=f'{optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:0.2e}')\n","        epoch_loss += loss.item()\n","    writer.add_scalar('Train/Loss', epoch_loss / len(train_loader), epoch)\n","\n","def valid_step(valid_loader, model, criterion, device, writer, epoch):\n","    pred_label_list = []\n","    label_list = []\n","    for i in range(3):\n","        pred_label_list.append(torch.zeros(CFG.shape_list[i]).to(device))\n","        label_list.append(torch.zeros(CFG.shape_list[i]).to(device))\n","    model.eval()\n","    epoch_loss = 0\n","    best_core = 0\n","    dice_scores = []\n","    iou_scores = []\n","\n","    for step, (images, labels, positions) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            y_preds = model(images)\n","            y_preds = resize(input=y_preds,\n","                            size=labels.shape[2:],\n","                            mode='bilinear',\n","                            align_corners=False)\n","            loss = criterion(y_preds, labels)\n","        # make whole mask\n","        y_preds = torch.sigmoid(y_preds)\n","        pred_img = (y_preds > CFG.threshhold).float()\n","        positions = positions.squeeze()\n","        for i in range(len(positions)):\n","            x1, y1, x2, y2, fragment_id = positions[i].numpy().tolist()\n","            pred_label_list[fragment_id][y1:y2, x1:x2] = pred_img[i].squeeze(0)\n","            label_list[fragment_id][y1:y2, x1:x2] = labels[i].squeeze(0)\n","        epoch_loss += loss.item()\n","        # 计算准确率\n","        dice_score = dice_coef(labels.to(device), y_preds.to(device), thr=CFG.threshhold).item()\n","        iou_socre = iou_coef(labels.to(device), y_preds.to(device), thr=CFG.threshhold).item()\n","\n","        dice_scores.append(dice_score)\n","        iou_scores.append(iou_socre)\n","                \n","    # 使用make_grid将图片转换成网格形式\n","    for i in range(len(pred_label_list)):\n","\n","        pred_mask = make_grid(pred_label_list[i].to(device), normalize=True)\n","        true_mask = make_grid(label_list[i].to(device), normalize=True)\n","        # 使用add_image方法将图片添加到TensorBoard中\n","        writer.add_image('Valid/' + str(i) + '_True_mask', true_mask, global_step=epoch, dataformats=\"CHW\")\n","        writer.add_image('Valid/' + str(i) + '_Pred_mask', pred_mask, global_step=epoch, dataformats=\"CHW\")\n","\n","    writer.add_scalar('Val/IOU', sum(iou_scores) / len(iou_scores), epoch)\n","    writer.add_scalar('Val/Dice', sum(dice_scores) / len(dice_scores), epoch)\n","    writer.add_scalar('Valid/Loss', epoch_loss / len(valid_loader), epoch)\n","    if epoch % 5 == 0 or sum(iou_scores) / len(iou_scores) > best_core:\n","        best_core = sum(iou_scores) / len(iou_scores)\n","        torch.save(model.state_dict(), 'result/' +  '{}-DIM-{}-[eval_loss]-{:.4f}-[dice_score]-{:.2f}-[iou_score]-{:.2f}-'.format(model_name, CFG.in_chans , epoch_loss / len(valid_loader), sum(dice_scores) / len(dice_scores), sum(iou_scores) / len(iou_scores)) + str(epoch) + '-epoch.pkl')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train:\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 86/782 [00:55<07:25,  1.56it/s, epoch=1, gpu_mem=8.29 GB, loss=0.5798, lr=1.00e-05]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(CFG\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     train_step(train_loader, model, criterion, optimizer, writer, CFG\u001b[39m.\u001b[39;49mdevice, i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mval:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     valid_step(valid_loader, model, criterion, CFG\u001b[39m.\u001b[39mdevice, writer, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n","Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(train_loader, model, criterion, optimizer, writer, device, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m resize(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39moutputs,\n\u001b[1;32m     10\u001b[0m                     size\u001b[39m=\u001b[39mlabel\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:],\n\u001b[1;32m     11\u001b[0m                     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m                     align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, label\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> 14\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), CFG\u001b[39m.\u001b[39mmax_grad_norm)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["criterion = smp.losses.SoftBCEWithLogitsLoss()\n","optimizer = optim.AdamW(model.parameters(),\n","                        lr=CFG.lr,\n","                        betas=(0.9, 0.999),\n","                        weight_decay=CFG.weight_decay\n","                        )\n","writer = SummaryWriter('result/logs')\n","for i in range(CFG.epochs):\n","    print('train:')\n","    train_step(train_loader, model, criterion, optimizer, writer, CFG.device, i + 1)\n","    print('val:')\n","    valid_step(valid_loader, model, criterion, CFG.device, writer, i + 1)"]}],"metadata":{"kernelspec":{"display_name":"dtt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"61166640a95ac08c4dfcc44984648f9045f44ea39ff0c31cc75bc4fde1586daf"}}},"nbformat":4,"nbformat_minor":4}
