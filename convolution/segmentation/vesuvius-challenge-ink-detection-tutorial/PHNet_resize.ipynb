{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a18c7d0f-a17a-4603-956d-2b531016c536","_uuid":"748d3554-7759-4443-b1b8-916e19ca50ed","trusted":true},"source":["This is a notebook explaining the [Ink Detection progress prize on Kaggle](https://www.kaggle.com/competitions/vesuvius-challenge), which is part of the larger [Vesuvius Challenge](https://scrollprize.org).\n","\n","For more background on the process of ink detection, be sure to check out [Tutorial 4: Ink Detection](https://scrollprize.org/tutorial4) on the Vesuvius Challenge website.\n","\n","In this notebook we'll see how to train a simple ML model to detect ink in a papyrus fragment from a 3d x-ray scan of the fragment.\n","\n","<img src=\"https://user-images.githubusercontent.com/177461/224853397-3cf86dc2-45b4-4e7c-9ec2-28a733791a75.jpg\" width=\"200\"/>\n","\n","First, initialize some variables, and let's look at a photo of the fragment. We won't use this for training, but it's useful to see.\n","\n","It's an infrared photo, since the ink is better visible in infrared light."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"ff3c9fb9-0c86-4acf-9162-c741c46e53a4","_kg_hide-output":false,"_uuid":"91db1348-a896-4607-8686-f6c6df6419ed","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:30:35.202851Z","iopub.status.busy":"2023-03-19T07:30:35.202431Z","iopub.status.idle":"2023-03-19T07:30:53.178442Z","shell.execute_reply":"2023-03-19T07:30:53.177132Z","shell.execute_reply.started":"2023-03-19T07:30:35.202813Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import glob\n","import PIL.Image as Image\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","from ipywidgets import interact, fixed\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","import os\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","\n","PREFIX = ['data/train/1/', 'data/train/2/', 'data/train/3/']\n","TEST_PREFIX = ['data/test/a/', 'data/test/b/']\n","BUFFER = 32  # Buffer size in x and y direction\n","Z_START = 25 # First slice in the z direction to use\n","Z_DIM = 16   # Number of slices in the z direction\n","TRAINING_STEPS = 60000\n","SHARED_HEIGHT = 3072\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE =  32\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IS_TRAIN = False\n","CHEPOINT = 'result/[train_loss]-0.0268-[dice_score]-0.1805-[iou_score]-0.0992-8-epoch.pkl'\n","FT = True # 是否加载预训练权重\n","THRESHOLD = 0.5 # mask阈值\n","\n","def resize(img):\n","    current_width, current_height = img.size\n","    aspect_ratio = current_width / current_height\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, SHARED_HEIGHT)\n","    img = img.resize(new_size)\n","    return img\n","def img_concat(img_list):\n","    width = []\n","    height = []\n","    for i in img_list:\n","        width.append(i.width)\n","        height.append(i.height)\n","    target = Image.new('RGB', (sum(width), SHARED_HEIGHT))  #拼接前需要写拼接完成后的图片大小\n","    for i in range(len(img_list)):\n","        x = sum(width[:i])            # 图片距离左边的大小\n","        y = 0                         # 图片距离上边的大小\n","        w = x + width[i]              # 图片距离左边的大小 + 图片自身宽度\n","        h = y + height[i]             # 图片距离上边的大小 + 图片自身高度\n","        target.paste(img_list[i], (x, y, w, h))\n","    return target\n","if IS_TRAIN:\n","    for i in range(len(PREFIX)):\n","        fig, ax1 = plt.subplots(1, 1)\n","        ax1.set_title(str(i) + \"_ir.png\")\n","        ax1.imshow(resize(Image.open(PREFIX[i] + 'ir.png')))\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"7607926b-5a4d-4107-8935-955288d53ebd","_uuid":"ee5f3f31-6cc0-4dbf-b4e1-e9afb18bc0ea","trusted":true},"source":["Let's load these binary images:\n","* **mask.png**: a mask of which pixels contain data, and which pixels we should ignore.\n","* **inklabels.png**: our label data: whether a pixel contains ink or no ink (which has been hand-labeled based on the infrared photo)."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"43a9acbe-f2c5-4976-b9ee-027e62c27a83","_uuid":"351fefa9-30dd-4e8d-bf3b-1aaa0fb33905","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:21:05.585716Z","iopub.status.busy":"2023-03-19T07:21:05.585127Z","iopub.status.idle":"2023-03-19T07:21:09.808558Z","shell.execute_reply":"2023-03-19T07:21:09.807356Z","shell.execute_reply.started":"2023-03-19T07:21:05.585678Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["if IS_TRAIN:\n","    # 加载训练数据\n","    mask_list = []\n","    lable_list = []\n","    for i in range(len(PREFIX)):\n","        mask = resize(Image.open(PREFIX[i]+\"mask.png\"))\n","        label = resize(Image.open(PREFIX[i]+\"inklabels.png\"))\n","        mask_list.append(mask)\n","        lable_list.append(label)\n","    mask_all = np.array(img_concat(mask_list).convert('1'))\n","    label_all = torch.from_numpy(np.array(img_concat(lable_list).convert('1'))).gt(0).float().to(DEVICE)\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    ax1.set_title(\"mask.png\")\n","    ax1.imshow(mask_all, cmap='gray')\n","    ax2.set_title(\"inklabels.png\")\n","    ax2.imshow(label_all.cpu(), cmap='gray')\n","    plt.show()\n","    del mask_list\n","    del lable_list"]},{"cell_type":"markdown","metadata":{"_cell_guid":"09e95f98-439b-49c3-aae2-550fadd533df","_uuid":"74a6ae91-2aa2-4eb3-9f3c-956dc54cf017","trusted":true},"source":["Next, we'll load the 3d x-ray of the fragment. This is represented as a .tif image stack. The image stack is an array of 16-bit grayscale images. Each image represents a \"slice\" in the z-direction, going from below the papyrus, to above the papyrus. We'll convert it to a 4D tensor of 32-bit floats. We'll also convert the pixel values to the range [0, 1].\n","\n","To save memory, we'll only load the innermost slices (`Z_DIM` of them). Let's look at them when we're done."]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"f75858f9-06ad-43cc-be5f-ab09738a58c1","_uuid":"929b9e7a-d30c-4462-a7b9-0765a76cdb4e","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:21:09.811169Z","iopub.status.busy":"2023-03-19T07:21:09.810453Z","iopub.status.idle":"2023-03-19T07:21:25.304029Z","shell.execute_reply":"2023-03-19T07:21:25.302929Z","shell.execute_reply.started":"2023-03-19T07:21:09.811130Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load the 3d x-ray scan, one slice at a time\n","if IS_TRAIN:\n","  image_stack_all = []\n","  for i in range(len(PREFIX)):\n","    # Load the 3d x-ray scan, one slice at a time\n","    z_slices = []\n","    for filename in tqdm(sorted(glob.glob(PREFIX[i]+\"surface_volume/*.tif\"))[Z_START:Z_START+Z_DIM]):\n","        img = Image.open(filename)\n","        img = resize(img)\n","        z_slice = torch.from_numpy(np.array(img, dtype=\"float32\")/65535.0)\n","        z_slices.append(z_slice)\n","    z_slices = torch.stack(z_slices, axis=-1)\n","    image_stack_all.append(z_slices)\n","  image_stack_all = torch.concat(image_stack_all, dim=1)\n","  fig, axes = plt.subplots(1, image_stack_all.shape[2])\n","  for z, ax in enumerate(axes):\n","    ax.imshow(image_stack_all[:, :, z], cmap='gray')\n","    ax.set_xticks([]); ax.set_yticks([])\n","  fig.tight_layout()\n","  plt.show()\n","  image_stack_all = image_stack_all.permute(2, 0, 1)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"173e0034-8ce4-4a12-9a0b-43cff0022249","_uuid":"18a40c3b-4241-4f8d-b808-e1cab35b8f6e","jupyter":{"source_hidden":true},"trusted":true},"source":["Can you see the ink in these slices of the 3d x-ray scan..? Neither can we.\n","\n","Now we'll create a dataset of subvolumes. We use a small rectangle around the letter \"P\" for our evaluation, and we'll exclude those pixels from the training set. (It's actually a Greek letter \"rho\", which looks similar to our \"P\".)"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"62d60bbe-85f1-4df4-b1e6-f4337268b11c","_uuid":"39a855d9-38f5-4f2d-ac7f-1f65529d1a3e","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:21:25.308039Z","iopub.status.busy":"2023-03-19T07:21:25.307245Z","iopub.status.idle":"2023-03-19T07:21:27.187141Z","shell.execute_reply":"2023-03-19T07:21:27.186043Z","shell.execute_reply.started":"2023-03-19T07:21:25.307998Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["if IS_TRAIN:\n","    rect = (3000, 1500, 800, 400)\n","    fig, ax = plt.subplots()\n","    ax.imshow(label_all.cpu())\n","    patch = patches.Rectangle((rect[0], rect[1]), rect[2], rect[3], linewidth=2, edgecolor='r', facecolor='none')\n","    ax.add_patch(patch)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"456fbd67-b8ed-4622-925a-3efc9fb7eb4a","_uuid":"4061cd69-3dd7-495c-838f-17280c1041b2","trusted":true},"source":["Now we'll define a PyTorch dataset and (super simple) model."]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"25930dc1-1883-4fcd-9d40-f4aaa2ca152b","_kg_hide-output":true,"_uuid":"9cf6abaa-b06b-4d05-a6d8-b7f012cf2e2c","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:21:27.189440Z","iopub.status.busy":"2023-03-19T07:21:27.188760Z","iopub.status.idle":"2023-03-19T07:21:27.197989Z","shell.execute_reply":"2023-03-19T07:21:27.196895Z","shell.execute_reply.started":"2023-03-19T07:21:27.189397Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def sample_random_location(shape):\n","    random_train_x = torch.random.uniform(shape=(), minval=BUFFER, maxval=shape[0] - BUFFER - 1, dtype=\"int32\")\n","    random_train_y = torch.random.uniform(shape=(), minval=BUFFER, maxval=shape[1] - BUFFER - 1, dtype=\"int32\")\n","    random_train_location = torch.stack([random_train_x, random_train_y])\n","    return random_train_location\n","class SubvolumeDataset(data.Dataset):\n","    def __init__(self, image_stack, label, pixels, is_train=False, is_val=False):\n","        self.image_stack = image_stack\n","        self.label = label\n","        self.pixels = pixels\n","        self.is_train = is_train\n","        self.is_val = is_val\n","    def __len__(self):\n","        return len(self.pixels)\n","    def __getitem__(self, index):\n","        # if self.is_train and not self.is_val:\n","        #     index = torch.randint(0, len(self.pixels) - 1,[1])\n","        #     y, x = self.pixels[index[0]]\n","        # else:\n","        y, x = self.pixels[index]\n","        subvolume = self.image_stack[:, y-BUFFER:y+BUFFER+1, x-BUFFER:x+BUFFER+1].view(1, Z_DIM, BUFFER*2+1, BUFFER*2+1)\n","        if self.is_train:\n","            inklabel = self.label[y, x].view(1)\n","            return subvolume, inklabel\n","        else:\n","            return subvolume\n","# IOU and Dice Score\n","def dice_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2 * inter + epsilon) / (den + epsilon)).mean()\n","    return dice\n","\n","\n","def iou_coef(y_true, y_pred, thr=0.5, dim=(0, 1), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred > thr).to(torch.float32)\n","    inter = (y_true * y_pred).sum(dim=dim)\n","    union = (y_true + y_pred - y_true * y_pred).sum(dim=dim)\n","    iou = ((inter + epsilon) / (union + epsilon)).mean()\n","    return iou\n","\n","# model = nn.Sequential(\n","#     nn.Conv3d(1, 16, 3, 1, 1), nn.MaxPool3d(2, 2),\n","#     nn.Conv3d(16, 32, 3, 1, 1), nn.MaxPool3d(2, 2),\n","#     nn.Conv3d(32, 64, 3, 1, 1), nn.MaxPool3d(2, 2),\n","#     nn.Flatten(start_dim=1),\n","#     nn.LazyLinear(128), nn.ReLU(),\n","#     nn.LazyLinear(1), nn.Sigmoid()\n","# ).to(DEVICE)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:21:27.201325Z","iopub.status.busy":"2023-03-19T07:21:27.200972Z","iopub.status.idle":"2023-03-19T07:21:27.221232Z","shell.execute_reply":"2023-03-19T07:21:27.220146Z","shell.execute_reply.started":"2023-03-19T07:21:27.201284Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/stu/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([32, 1])\n"]}],"source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.model_zoo as model_zoo\n","import torchvision.models as models\n","FRAMES_PER_IMG = 3\n","\n","\n","class PHNet(nn.Module):\n","    def __init__(self,load_weights=False):\n","        super(PHNet, self).__init__()\n","        self.seen = 0\n","        self.frame = Z_DIM\n","        self.CRPool = nn.Conv3d(1, 3, kernel_size=(2, 3, 3), stride=1, padding=(1, 1, 1), bias=False)\n","        self.conv11 = nn.Conv3d(in_channels=3, out_channels=3, kernel_size=(self.frame + 1,1,1), stride=1)\n","        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512]\n","        self.frontend_feat2 = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512]\n","        self.backend_feat  = [512, 256, 128, 64]\n","        self.backend_feat2  = [512, 256, 128]\n","        self.frontend = make_layers(self.frontend_feat, in_channels=1)\n","        self.frontend2 = make_layers(self.frontend_feat)\n","        self.backend = make_layers(self.backend_feat, in_channels = 512, dilation = 2)\n","        self.backend2 = make_layers(self.backend_feat2, in_channels = 512, dilation = 2)\n","        self.pool = nn.MaxPool2d(2)\n","        self.flatten = nn.Flatten()\n","        self.linear1 = nn.LazyLinear(256)\n","        self.relu1 = nn.ReLU()\n","        self.linear2 = nn.LazyLinear(128)\n","        self.relu2 = nn.ReLU()\n","        self.linear3 = nn.LazyLinear(1)\n","        self.sigmoid = nn.Sigmoid()\n","        # self.output_layer = nn.Conv2d(192, 1, kernel_size=1)\n","        # self.relu = nn.ReLU()\n","        if not load_weights:\n","            # mod = models.resnet34(pretrained=True)\n","            self._initialize_weights()\n","            #num = 0\n","            # for i in range(len(self.frontend.state_dict().items())):\n","            #     list(self.frontend.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]\n","            #     list(self.frontend2.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]\n","\n","    def forward(self,x):\n","        y = x.clone()\n","        y = self.CRPool(y)\n","        y = self.conv11(y*y)\n","        y = torch.squeeze(y, dim = 2)\n","        y = self.frontend2(y)\n","        y = self.backend2(y)\n","        x = x[:,:,-1,:,:] \n","        x = self.frontend(x)\n","        x = self.backend(x)\n","        x = torch.cat([x,y], dim=1)\n","        # x = self.output_layer(x)\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","        x = self.linear1(x)\n","        x = self.relu1(x)\n","        x = self.linear2(x)\n","        x = self.relu2(x)\n","        x = self.linear3(x)\n","        x = self.sigmoid(x)\n","        return x\n","        \n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.normal_(m.weight, std=0.01)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif (isinstance(m, nn.Conv3d)):\n","                if m.bias is not None:\n","                    nn.init.normal_(m.weight, std=0.01)\n","                else:\n","                    nn.init.constant_(m.weight, float(1/18/self.frame))\n","\n","\n","def make_layers(cfg, in_channels = 3,batch_norm=True,dilation = False):\n","    if dilation:\n","        d_rate = 2\n","    else:\n","        d_rate = 1\n","    layers = []\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","                #layers.append(CBAM(v, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False))\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","def main():\n","    model=PHNet()\n","    x = torch.rand(32, 1, Z_DIM, 64, 64)\n","    x = model(x)\n","    print(x.shape)\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e7b84e1e-7903-4290-9e09-3bf2cb62b222","_uuid":"cfbc9413-b6ff-4a69-ae60-22dddef18f11","trusted":true},"source":["Now we'll train the model. Conceptually it looks like this:\n","\n","<a href=\"https://user-images.githubusercontent.com/22727759/224853655-3fad9edb-c798-452e-94d0-f74efe71c08e.mp4\"><img src=\"https://user-images.githubusercontent.com/22727759/224853385-ed190d89-f466-469c-82a9-499881759d57.gif\"/></a>\n","\n","This typically takes about 10 minutes."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:21:27.223154Z","iopub.status.busy":"2023-03-19T07:21:27.222612Z","iopub.status.idle":"2023-03-19T07:22:04.777006Z","shell.execute_reply":"2023-03-19T07:22:04.775953Z","shell.execute_reply.started":"2023-03-19T07:21:27.223117Z"},"trusted":true},"outputs":[],"source":["if IS_TRAIN:    \n","    print(\"Generating pixel lists of train...\")\n","    # Split our dataset into train and val. The pixels inside the rect are the \n","    # val set, and the pixels outside the rect are the train set.\n","    pixels_inside_rect_all = []\n","    pixels_outside_rect_all = []\n","    for pixel in zip(*np.where(mask_all == 1)):\n","        if pixel[1] < BUFFER or pixel[1] >= mask_all.shape[1]-BUFFER or pixel[0] < BUFFER or pixel[0] >= mask_all.shape[0]-BUFFER:\n","            continue # Too close to the edge\n","        if pixel[1] >= rect[0] and pixel[1] <= rect[0]+rect[2] and pixel[0] >= rect[1] and pixel[0] <= rect[1]+rect[3]:\n","            pixels_inside_rect_all.append(pixel)\n","        else:\n","            pixels_outside_rect_all.append(pixel)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T07:22:04.779128Z","iopub.status.busy":"2023-03-19T07:22:04.778480Z","iopub.status.idle":"2023-03-19T07:22:06.646772Z","shell.execute_reply":"2023-03-19T07:22:06.645661Z","shell.execute_reply.started":"2023-03-19T07:22:04.779088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint loaded\n"]}],"source":["# model = ResNet3D(block=ResNetBlock, layers=[1, 1, 1, 1], num_classes=1).to(DEVICE)\n","# model = ResNet3DLess(block=ResNetBlock, layers=[1, 2], num_classes=1).to(DEVICE)\n","model=PHNet().to(DEVICE)\n","model_name = 'PHNet'\n","if FT:\n","    try:\n","        checkpoint = torch.load(CHEPOINT, map_location=DEVICE)\n","        models_dict = model.state_dict()\n","        for model_part in models_dict:\n","            if model_part in checkpoint:\n","                models_dict[model_part] = checkpoint[model_part]\n","        model.load_state_dict(models_dict)\n","        print('Checkpoint loaded')\n","    except:\n","        print('Checkpoint not loaded')\n","        pass"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"790108d2-5260-4276-9105-9da5a27c28f5","_uuid":"29fb8028-1ec5-42fb-b61a-658e1181c951","execution":{"iopub.execute_input":"2023-03-19T07:22:06.650589Z","iopub.status.busy":"2023-03-19T07:22:06.649965Z","iopub.status.idle":"2023-03-19T07:22:06.662667Z","shell.execute_reply":"2023-03-19T07:22:06.661396Z","shell.execute_reply.started":"2023-03-19T07:22:06.650559Z"},"trusted":true},"outputs":[],"source":["if IS_TRAIN:\n","    # 实例化SummaryWriter对象\n","    torch.cuda.empty_cache()\n","    writer = SummaryWriter('result/logs')\n","    EPOCH =  30\n","    T_max = int(30000 / BATCH_SIZE * EPOCH) + 50\n","    min_lr = 0.000001\n","    print('''\n","    Starting training:\n","        Model: {}\n","        Epochs: {}\n","        Batch size: {}\n","        Learning rate: {}\n","        Training Step: {}\n","        CUDA: {}\n","    '''.format(model_name,\n","               EPOCH,\n","               BATCH_SIZE,\n","               LEARNING_RATE,\n","               TRAINING_STEPS,\n","               DEVICE.type))\n","    criterion = nn.BCELoss()\n","    optimizer = optim.AdamW(model.parameters(),\n","                            lr=LEARNING_RATE,\n","                            betas=(0.9, 0.999),\n","                            weight_decay=0.001\n","                            )\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=min_lr)\n","    max_memory = torch.cuda.max_memory_allocated(device=DEVICE) / 1E9 if torch.cuda.is_available() else 0\n","    # epoch_bar = tqdm(range(EPOCH), total=EPOCH)\n","    iter = 0\n","    for epoch in range(EPOCH):\n","        # 循环训练 1~3中的数据，每轮数据只抽取了TRAINING_STEPS的长度，也可以全部加入\n","        # 加载数据\n","        train_dataset = SubvolumeDataset(image_stack_all, label_all, pixels_outside_rect_all, IS_TRAIN)\n","        eval_dataset = SubvolumeDataset(image_stack_all, label_all, pixels_inside_rect_all, IS_TRAIN, is_val=True)\n","        train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","        eval_loader = data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","        epoch_loss = 0\n","        model.train()\n","        bar = tqdm(enumerate(train_loader), total=TRAINING_STEPS) \n","        for i, (subvolumes, inklabels) in bar:\n","            if i > TRAINING_STEPS:\n","                break\n","            optimizer.zero_grad()\n","            outputs = model(subvolumes.to(DEVICE))\n","            loss = criterion(outputs, inklabels.to(DEVICE))\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","            bar.set_postfix(loss=f'{loss.item():0.4f}', epoch=epoch ,gpu_mem=f'{mem:0.2f} GB')\n","            epoch_loss += loss.item()\n","        writer.add_scalar('Train/Loss', epoch_loss / TRAINING_STEPS, epoch)\n","        output = torch.zeros_like(label_all).float()\n","        true = torch.zeros_like(label_all).float()\n","        model.eval()\n","        with torch.no_grad():\n","            for i, (subvolumes, inklabels) in enumerate(tqdm(eval_loader)):\n","                outputs = model(subvolumes.to(DEVICE))\n","                for j, (value, true_value) in enumerate(zip(outputs, inklabels)):\n","                    output[pixels_inside_rect_all[i*BATCH_SIZE+j]] = value\n","                    true[pixels_inside_rect_all[i*BATCH_SIZE+j]] = true_value\n","\n","            # 计算准确率\n","            dice_score = dice_coef(true.to(DEVICE), output.to(DEVICE), thr=THRESHOLD).item()\n","            iou_socre = iou_coef(true.to(DEVICE), output.to(DEVICE), thr=THRESHOLD).item()\n","                    \n","            # 使用make_grid将图片转换成网格形式\n","            pred_mask = make_grid(output.to(DEVICE), normalize=True)\n","            true_mask = make_grid(true.to(DEVICE), normalize=True)\n","            # 使用add_image方法将图片添加到TensorBoard中\n","            writer.add_image('Valid/True_mask', true_mask, global_step=epoch, dataformats=\"CHW\")\n","            writer.add_image('Valid/Pred_mask', pred_mask, global_step=epoch, dataformats=\"CHW\")\n","\n","            # fig, (ax1, ax2) = plt.subplots(1, 2)\n","            # ax1.imshow(output.cpu(), cmap='gray')\n","            # ax2.imshow(label.cpu(), cmap='gray')\n","            # plt.show()\n","            writer.add_scalar('Val/IOU', iou_socre,  epoch)\n","            writer.add_scalar('Val/Dice', dice_score,  epoch)\n","            import gc\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            torch.save(model.state_dict(), 'result/' +  '{}-DIM-{}-[train_loss]-{:.2f}-[dice_score]-{:.2f}-[iou_score]-{:.2f}-'.format(model_name, Z_DIM ,epoch_loss / TRAINING_STEPS, dice_score, iou_socre) + str(iter) + '-epoch.pkl')\n","    writer.close()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"3c6ad763-f47c-4aaa-8c12-cb95c2d28d74","_uuid":"8edfa121-b2ef-419e-acbe-6d430fb50133","trusted":true},"source":["Finally, we'll generate a prediction image. We'll use the model to predict the presence of ink for each pixel in our rectangle (the val set). Conceptually it looks like this:\n","\n","<a href=\"https://user-images.githubusercontent.com/22727759/224853653-7cffd0a4-c6fa-49a2-93c1-e3c820863a51.mp4\"><img src=\"https://user-images.githubusercontent.com/22727759/224853379-09ae991e-02be-4ecc-a652-313165b3005c.gif\"/></a>\n","\n","\n","This should take about a minute.\n","\n","Remember that the model has never seen the label data within the rectangle before!\n","\n","We'll plot it side-by-side with the label image. Are you able to recognize the letter \"P\" in it?"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 16/16 [00:03<00:00,  4.62it/s]\n","100%|██████████| 16/16 [00:03<00:00,  4.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating pixel lists of test...\n"]}],"source":["# 加载测试数据\n","shape_list = []\n","test_mask_list = []\n","for i in range(len(TEST_PREFIX)):\n","    shape_list.append(torch.from_numpy(np.array(resize(Image.open(TEST_PREFIX[i] + \"mask.png\")))))\n","    test_mask_list.append(np.array(resize(Image.open(TEST_PREFIX[i] + \"mask.png\")).convert('1')))\n","\n","test_images_list = []\n","test_image_stack_list = []\n","for i in range(len(TEST_PREFIX)):\n","    images = [np.array(resize(Image.open(filename)), dtype=np.float32)/65535.0 for filename in tqdm(sorted(glob.glob(TEST_PREFIX[i]+\"surface_volume/*.tif\"))[Z_START:Z_START+Z_DIM])]\n","    images_stack = torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n","    test_images_list.append(images)\n","    test_image_stack_list.append(images_stack)\n","\n","print(\"Generating pixel lists of test...\")\n","# test不需要分割，取mask内的全部\n","pixels_test_rect_list = []\n","for i in range(len(test_mask_list)):\n","    pixels_test_rect = []\n","    for pixel in zip(*np.where(test_mask_list[i] == 1)):\n","        if pixel[1] < BUFFER or pixel[1] >= test_mask_list[i].shape[1]-BUFFER or pixel[0] < BUFFER or pixel[0] >= test_mask_list[i].shape[0]-BUFFER:\n","            continue # Too close to the edge\n","        pixels_test_rect.append(pixel)\n","    pixels_test_rect_list.append(pixels_test_rect)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"56812c41-904d-4645-bddf-49b19fe2685d","_uuid":"68013338-2e52-4f0b-b15b-b18e071aa5da","collapsed":false,"execution":{"iopub.execute_input":"2023-03-19T07:25:33.475121Z","iopub.status.busy":"2023-03-19T07:25:33.474728Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 2613/424902 [01:41<4:33:11, 25.76it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m i, (subvolumes) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(test_eval_loader)):\n\u001b[0;32m----> 9\u001b[0m         \u001b[39mfor\u001b[39;00m j, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model(subvolumes\u001b[39m.\u001b[39;49mto(DEVICE))):\n\u001b[1;32m     10\u001b[0m             output[pixels_test_rect_list[index][i\u001b[39m*\u001b[39mBATCH_SIZE\u001b[39m+\u001b[39mj]] \u001b[39m=\u001b[39m value\n\u001b[1;32m     11\u001b[0m     output_list\u001b[39m.\u001b[39mappend(output)\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mPHNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv11(y\u001b[39m*\u001b[39my)\n\u001b[1;32m     46\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(y, dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfrontend2(y)\n\u001b[1;32m     48\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend2(y)\n\u001b[1;32m     49\u001b[0m x \u001b[39m=\u001b[39m x[:,:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:] \n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["output_list = []\n","for index in range(len(test_image_stack_list)):\n","    test_dataset = SubvolumeDataset(test_image_stack_list[index], None, pixels_test_rect_list[index], IS_TRAIN)\n","    test_eval_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","    output = torch.zeros_like(shape_list[index]).float()\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (subvolumes) in enumerate(tqdm(test_eval_loader, total=TRAINING_STEPS)):\n","            if i > TRAINING_STEPS:\n","                break\n","            for j, value in enumerate(model(subvolumes.to(DEVICE))):\n","                output[pixels_test_rect_list[index][i*BATCH_SIZE+j]] = value\n","        output_list.append(output)\n","        out = output_list[index].gt(THRESHOLD).cpu().float().numpy()\n","        import cv2\n","        cv2.imwrite(str(index + 1) + '.png', out * 255)\n","# reshape\n","for i in range(len(output_list)):\n","    org = Image.open(TEST_PREFIX[i] + 'mask.png')\n","    width, height = org.width, org.height\n","    output_list[i].reshape(width, height)\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.imshow(output_list[0].gt(THRESHOLD).cpu(), cmap='gray')\n","ax1.imshow(output_list[1].gt(THRESHOLD).cpu(), cmap='gray')\n","plt.show()  "]},{"cell_type":"markdown","metadata":{"_cell_guid":"49b12c62-7143-4d27-be79-e20e8cd9f5fe","_uuid":"15c0a510-b4d1-4e14-974e-cbe5a7ac6b8e","trusted":true},"source":["Since our output has to be binary, we have to choose a threshold, say 40% confidence."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f1a3ed4-43c8-4a9c-9f49-dab4d3047048","_uuid":"3eb989fa-2823-49f7-a55f-1e178884e344","collapsed":false,"execution":{"iopub.status.busy":"2023-03-19T07:22:28.186015Z","iopub.status.idle":"2023-03-19T07:22:28.186806Z","shell.execute_reply":"2023-03-19T07:22:28.186565Z","shell.execute_reply.started":"2023-03-19T07:22:28.186538Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["if IS_TRAIN:\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    ax1.imshow(output.gt(THRESHOLD).cpu(), cmap='gray')\n","    ax2.imshow(label.cpu(), cmap='gray')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"df73edb8-b09d-4e84-b33a-b384dbe486fe","_uuid":"04dc6e9a-5178-4ffa-95b3-a64783d4cf1a","trusted":true},"source":["Finally, Kaggle expects a runlength-encoded submission.csv file, so let's output that."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79b4b495-2e93-49ba-b9d8-434dccf49907","_uuid":"512cd6ab-2794-4ad3-87cc-fc240561f286","collapsed":false,"execution":{"iopub.status.busy":"2023-03-19T07:22:28.188296Z","iopub.status.idle":"2023-03-19T07:22:28.189057Z","shell.execute_reply":"2023-03-19T07:22:28.188810Z","shell.execute_reply.started":"2023-03-19T07:22:28.188784Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def rle(output):\n","    flat_img = np.where(output.flatten().cpu() > THRESHOLD, 1, 0).astype(np.uint8)\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n","# rle_output = rle(output)\n","# This doesn't make too much sense, but let's just output in the required format\n","# so notebook works as a submission. :-)\n","# print(\"Id,Predicted\\na,\" + rle_output + \"\\nb,\" + rle_output, file=open('submission.csv', 'w'))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"799ddbd9-6862-4a43-9ae4-3c2d2f01da85","_uuid":"e84d0aa9-a297-4a90-b4f2-a8afb7c389c5","trusted":true},"source":["Hurray! We've detected ink! Now, can you do better? :-) For example, you could start with this [example submission](https://www.kaggle.com/code/danielhavir/vesuvius-challenge-example-submission)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-03-19T07:22:28.190428Z","iopub.status.idle":"2023-03-19T07:22:28.191183Z","shell.execute_reply":"2023-03-19T07:22:28.190951Z","shell.execute_reply.started":"2023-03-19T07:22:28.190926Z"},"trusted":true},"outputs":[],"source":["rle_list = []\n","for output in outputs:\n","    rle_sample = rle(output)\n","    rle_list.append(rle_sample)\n","print(\"Id,Predicted\\na,\" + rle_list[0] + \"\\nb,\" + rle_list[1], file=open('submission.csv', 'w'))"]}],"metadata":{"kernelspec":{"display_name":"dtt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"61166640a95ac08c4dfcc44984648f9045f44ea39ff0c31cc75bc4fde1586daf"}}},"nbformat":4,"nbformat_minor":4}
