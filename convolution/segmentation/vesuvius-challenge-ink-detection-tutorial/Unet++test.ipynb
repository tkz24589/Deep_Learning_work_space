{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:20.589999Z","iopub.status.busy":"2023-04-05T10:30:20.589671Z","iopub.status.idle":"2023-04-05T10:30:20.610448Z","shell.execute_reply":"2023-04-05T10:30:20.609390Z","shell.execute_reply.started":"2023-04-05T10:30:20.589965Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","from torch.utils.data import ConcatDataset\n","import os\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import warnings\n","import gc\n","import cv2\n","# 忽略所有警告\n","warnings.filterwarnings('ignore')\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","seed_value = 42   # 设定随机数种子\n","\n","np.random.seed(seed_value)\n","random.seed(seed_value)\n","os.environ['PYTHONHASHSEED'] = str(seed_value)  # 为了禁止hash随机化，使得实验可复现。\n","\n","torch.manual_seed(seed_value)     # 为CPU设置随机种子\n","torch.cuda.manual_seed(seed_value)      # 为当前GPU设置随机种子（只用一块GPU）\n","torch.cuda.manual_seed_all(seed_value)   # 为所有GPU设置随机种子（多块GPU）\n","\n","torch.backends.cudnn.deterministic = True\n","\n","class CFG:\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    checpoint = '/kaggle/input/pretrain-unet/UnetPlusPlus-DIM-10-eval_loss-0.2591-dice_score-0.52-iou_score-0.40-25-epoch.pkl'\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","\n","    # # comp_dir_path = './'\n","    # comp_dir_path = '/kaggle/input/'\n","    # comp_folder_name = 'vesuvius-challenge'\n","    # # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    # comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","        # comp_dir_path = './'\n","    comp_dir_path = ''\n","    comp_folder_name = 'data'\n","    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","    \n","    exp_name = 'Unet++_stride'\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'Unet++'\n","\n","    in_chans = 10 # 65\n","    # ============== training cfg =============\n","    size = 224\n","    tile_size = 224\n","    stride = tile_size // 2\n","\n","    train_batch_size = 32 # 32\n","    valid_batch_size = 32\n","\n","    epochs = 50 # 30\n","\n","    # lr = 1e-4 / warmup_factor\n","    lr = 1e-4\n","\n","    # ============== fixed =============\n","    pretrained = True\n","\n","    min_lr = 1e-6\n","    weight_decay = 1e-6\n","    max_grad_norm = 1000\n","\n","    num_workers = 0\n","\n","    seed = 42\n","\n","    threshhold = 0.5\n","\n","    shape_list = []\n","    test_shape_list = []\n","\n","    # ============== set dataset path =============\n","\n","    # outputs_path = f'/kaggle/working/outputs/{comp_name}/{exp_name}/'\n","    outputs_path = 'result/'\n","\n","    submission_dir = outputs_path + 'submissions/'\n","    submission_path = submission_dir + f'submission_{exp_name}.csv'\n","\n","    model_dir = outputs_path\n","    log_dir = outputs_path + 'logs/'\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        # A.RandomResizedCrop(\n","        #     size, size, scale=(0.85, 1.0)),\n","        A.Resize(size, size),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.75),\n","        A.ShiftScaleRotate(p=0.75),\n","        A.OneOf([\n","                A.GaussNoise(var_limit=[10, 50]),\n","                A.GaussianBlur(),\n","                A.MotionBlur(),\n","                ], p=0.4),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                        mask_fill_value=0, p=0.5),\n","        # A.Cutout(max_h_size=int(size * 0.6),\n","        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n","        A.Normalize(\n","            mean= [0] * in_chans,\n","            std= [1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","\n","    valid_aug_list = [\n","        A.Resize(size, size),\n","        A.Normalize(\n","            mean= [0] * in_chans,\n","            std= [1] * in_chans\n","        ),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:20.612569Z","iopub.status.busy":"2023-04-05T10:30:20.612112Z","iopub.status.idle":"2023-04-05T10:30:20.620362Z","shell.execute_reply":"2023-04-05T10:30:20.619323Z","shell.execute_reply.started":"2023-04-05T10:30:20.612522Z"},"trusted":true},"outputs":[],"source":["def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","    return aug"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:34:26.880447Z","iopub.status.busy":"2023-04-05T10:34:26.880027Z","iopub.status.idle":"2023-04-05T10:34:26.892577Z","shell.execute_reply":"2023-04-05T10:34:26.891430Z","shell.execute_reply.started":"2023-04-05T10:34:26.880404Z"},"trusted":true},"outputs":[],"source":["class SubvolumeDataset(data.Dataset):\n","    def __init__(self, images, labels, positions, transform, is_train):\n","        self.transform = transform\n","        self.images = images\n","        self.labels = labels\n","        self.is_train = is_train\n","        self.positions = positions\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, index):\n","        if self.is_train:\n","            image = self.images[index]\n","            label = self.labels[index]\n","            if self.positions:\n","                position = np.array(self.positions[index])\n","            else:\n","                position = np.zeros(1)\n","            if self.transform:\n","                data = self.transform(image=image, mask=label)\n","                image = data['image']\n","                label = data['mask']\n","            return image, label, position\n","        else:\n","            image = self.images[index]\n","            position = np.array(self.positions[index])\n","            if self.transform:\n","                data = self.transform(image=image, mask=np.zeros(image.shape))\n","                image = data['image']\n","            return image, position"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:20.636235Z","iopub.status.busy":"2023-04-05T10:30:20.635882Z","iopub.status.idle":"2023-04-05T10:30:21.415959Z","shell.execute_reply":"2023-04-05T10:30:21.414780Z","shell.execute_reply.started":"2023-04-05T10:30:20.636198Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint not loaded\n"]}],"source":["# model = Ringed_Res_Unet(n_channels=CFG.in_chans, n_classes=CFG.target_size).to(CFG.device)\n","model = smp.UnetPlusPlus(in_channels=CFG.in_chans, classes=1).to(CFG.device)\n","# x = np.zeros((16, CFG.in_chans, 224, 224))\n","# x = torch.from_numpy(x).to(CFG.device).float()\n","# out = model(x)\n","# print(out.shape)\n","model_name = 'UnetPlusPlus'\n","if CFG.pretrained:\n","    try:\n","        checkpoint = torch.load(CFG.checpoint, map_location=CFG.device)\n","        models_dict = model.state_dict()\n","        for model_part in models_dict:\n","            if model_part in checkpoint:\n","                models_dict[model_part] = checkpoint[model_part]\n","        model.load_state_dict(models_dict)\n","        print('Checkpoint loaded')\n","    except:\n","        print('Checkpoint not loaded')\n","        pass"]},{"cell_type":"markdown","metadata":{"_cell_guid":"df73edb8-b09d-4e84-b33a-b384dbe486fe","_uuid":"04dc6e9a-5178-4ffa-95b3-a64783d4cf1a","trusted":true},"source":["Finally, Kaggle expects a runlength-encoded submission.csv file, so let's output that."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:21.418097Z","iopub.status.busy":"2023-04-05T10:30:21.417601Z","iopub.status.idle":"2023-04-05T10:30:21.428078Z","shell.execute_reply":"2023-04-05T10:30:21.427021Z","shell.execute_reply.started":"2023-04-05T10:30:21.418053Z"},"trusted":true},"outputs":[],"source":["def test_step(test_loader, model, device):\n","    pred_label_list = []\n","    for i in range(2):\n","        pred_label_list.append(torch.zeros(CFG.test_shape_list[i]).to(device))\n","    model.eval()\n","\n","    for step, (images, positions) in tqdm(enumerate(test_loader), total=len(test_loader)):\n","        images = images.to(device)\n","        with torch.no_grad():\n","            y_preds = model(images)\n","        # make whole mask\n","        y_preds = torch.sigmoid(y_preds)\n","        positions = positions.squeeze()\n","        for i in range(len(positions)):\n","            x1, y1, x2, y2, fragment_id = positions[i].numpy().tolist()\n","            pred_label_list[fragment_id][y1:y2, x1:x2] = y_preds[i].squeeze(0)\n","    return pred_label_list"]},{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"79b4b495-2e93-49ba-b9d8-434dccf49907","_uuid":"512cd6ab-2794-4ad3-87cc-fc240561f286","collapsed":false,"execution":{"iopub.execute_input":"2023-04-05T10:30:21.430572Z","iopub.status.busy":"2023-04-05T10:30:21.429817Z","iopub.status.idle":"2023-04-05T10:30:21.438847Z","shell.execute_reply":"2023-04-05T10:30:21.437766Z","shell.execute_reply.started":"2023-04-05T10:30:21.430524Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def rle(output):\n","    flat_img = np.where(output.flatten().cpu() > CFG.threshhold, 1, 0).astype(np.uint8)\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n","# rle_output = rle(output)\n","# This doesn't make too much sense, but let's just output in the required format\n","# so notebook works as a submission. :-)\n","# print(\"Id,Predicted\\na,\" + rle_output + \"\\nb,\" + rle_output, file=open('submission.csv', 'w'))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"799ddbd9-6862-4a43-9ae4-3c2d2f01da85","_uuid":"e84d0aa9-a297-4a90-b4f2-a8afb7c389c5","trusted":true},"source":["Hurray! We've detected ink! Now, can you do better? :-) For example, you could start with this [example submission](https://www.kaggle.com/code/danielhavir/vesuvius-challenge-example-submission)."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:21.440857Z","iopub.status.busy":"2023-04-05T10:30:21.440355Z","iopub.status.idle":"2023-04-05T10:30:21.453844Z","shell.execute_reply":"2023-04-05T10:30:21.452917Z","shell.execute_reply.started":"2023-04-05T10:30:21.440820Z"},"trusted":true},"outputs":[],"source":["def read_image_mask_test(fragment_id):\n","\n","    images = []\n","\n","    # idxs = range(65)\n","    mid = 65 // 2\n","    start = mid - CFG.in_chans // 2\n","    end = mid + CFG.in_chans // 2\n","    idxs = range(start, end)\n","\n","    for i in tqdm(idxs):\n","        \n","        image = cv2.imread(CFG.comp_dataset_path + f\"test/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n","\n","        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size + 1)\n","        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size + 1)\n","\n","        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","        images.append(image)\n","    images = np.stack(images, axis=2)\n","\n","    mask = cv2.imread(CFG.comp_dataset_path + f\"test/{fragment_id}/mask.png\", 0)\n","    mask = np.pad(mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    mask = mask.astype('float32')\n","    mask /= 255.0\n","\n","    CFG.test_shape_list.append(mask.shape)\n","    \n","    return images, mask"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:30:21.455880Z","iopub.status.busy":"2023-04-05T10:30:21.455221Z","iopub.status.idle":"2023-04-05T10:30:21.465262Z","shell.execute_reply":"2023-04-05T10:30:21.464111Z","shell.execute_reply.started":"2023-04-05T10:30:21.455841Z"},"trusted":true},"outputs":[],"source":["def get_test_dataset():\n","    test_images = []\n","    test_positons = []\n","    for fragment_id in ['a', 'b']:\n","\n","        if fragment_id == 'a':\n","            id = 0\n","        else:\n","            id = 1\n","\n","        image, mask = read_image_mask_test(fragment_id)\n","\n","        x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n","        y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n","\n","        for y1 in y1_list:\n","            for x1 in x1_list:\n","                y2 = y1 + CFG.tile_size\n","                x2 = x1 + CFG.tile_size\n","                if sum(list(mask[y1:y2, x1:x2].flatten())) != 0:\n","                    test_images.append(image[y1:y2, x1:x2])\n","                    test_positons.append([x1, y1, x2, y2, id])\n","   \n","    return test_images, test_positons"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 19.07it/s]\n","100%|██████████| 10/10 [00:01<00:00,  9.28it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_images, test_position \u001b[39m=\u001b[39m get_test_dataset()\n","Cell \u001b[0;32mIn[30], line 20\u001b[0m, in \u001b[0;36mget_test_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m y2 \u001b[39m=\u001b[39m y1 \u001b[39m+\u001b[39m CFG\u001b[39m.\u001b[39mtile_size\n\u001b[1;32m     19\u001b[0m x2 \u001b[39m=\u001b[39m x1 \u001b[39m+\u001b[39m CFG\u001b[39m.\u001b[39mtile_size\n\u001b[0;32m---> 20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msum\u001b[39;49m(\u001b[39mlist\u001b[39;49m(mask[y1:y2, x1:x2]\u001b[39m.\u001b[39;49mflatten())) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     test_images\u001b[39m.\u001b[39mappend(image[y1:y2, x1:x2])\n\u001b[1;32m     22\u001b[0m     test_positons\u001b[39m.\u001b[39mappend([x1, y1, x2, y2, \u001b[39mid\u001b[39m])\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["test_images, test_position = get_test_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:34:31.201635Z","iopub.status.busy":"2023-04-05T10:34:31.201226Z","iopub.status.idle":"2023-04-05T10:34:32.477679Z","shell.execute_reply":"2023-04-05T10:34:32.474809Z","shell.execute_reply.started":"2023-04-05T10:34:31.201599Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/83 [00:04<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 1.90 GiB already allocated; 170.56 MiB free; 2.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m test_dataset \u001b[39m=\u001b[39m SubvolumeDataset(test_images, \u001b[39mNone\u001b[39;00m, test_position, get_transforms(data\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m, cfg\u001b[39m=\u001b[39mCFG), \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(test_dataset,\n\u001b[1;32m      4\u001b[0m                             batch_size\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mvalid_batch_size,\n\u001b[1;32m      5\u001b[0m                             shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                             num_workers\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mnum_workers, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[39m=\u001b[39m test_step(test_loader, model, CFG\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i, output \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(outputs):\n\u001b[1;32m      9\u001b[0m     cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mstr\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m, output \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\n","Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mtest_step\u001b[0;34m(test_loader, model, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     y_preds \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     11\u001b[0m \u001b[39m# make whole mask\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(y_preds)\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/segmentation_models_pytorch/base/model.py:30\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[1;32m     29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m---> 30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39m*\u001b[39;49mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_head \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/segmentation_models_pytorch/decoders/unetplusplus/decoder.py:135\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    133\u001b[0m             cat_features \u001b[39m=\u001b[39m [dense_x[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_l_i\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(depth_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, dense_l_i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[1;32m    134\u001b[0m             cat_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(cat_features \u001b[39m+\u001b[39m [features[dense_l_i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m             dense_x[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00mdepth_idx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_l_i\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdepth_idx\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdense_l_i\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m](\n\u001b[1;32m    136\u001b[0m                 dense_x[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdepth_idx\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdense_l_i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m], cat_features\n\u001b[1;32m    137\u001b[0m             )\n\u001b[1;32m    138\u001b[0m dense_x[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m](dense_x[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m dense_x[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/dtt/lib/python3.9/site-packages/segmentation_models_pytorch/decoders/unetplusplus/decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m skip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x, skip], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 1.90 GiB already allocated; 170.56 MiB free; 2.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["rle_list = []\n","test_dataset = SubvolumeDataset(test_images, None, test_position, get_transforms(data='valid', cfg=CFG), False)\n","test_loader = data.DataLoader(test_dataset,\n","                            batch_size=CFG.valid_batch_size,\n","                            shuffle=False,\n","                            num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","outputs = test_step(test_loader, model, CFG.device)\n","for i, output in enumerate(outputs):\n","    cv2.imwrite(str(i + 1) + '.png', output * 255)\n","    rle_sample = rle(output)\n","    rle_list.append(rle_sample)\n","print(\"Id,Predicted\\na,\" + rle_list[0] + \"\\nb,\" + rle_list[1], file=open('submission.csv', 'w'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"61166640a95ac08c4dfcc44984648f9045f44ea39ff0c31cc75bc4fde1586daf"}}},"nbformat":4,"nbformat_minor":4}
